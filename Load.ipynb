{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e22d77dc-fb38-4420-ba7d-429f04b87ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spark = org.apache.spark.sql.SparkSession@68a03605\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@68a03605"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.feature.{StringIndexer, OneHotEncoder, VectorAssembler}\n",
    "import org.apache.spark.ml.regression.GeneralizedLinearRegression\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "//create spark session\n",
    "val spark = SparkSession.builder.appName(\"SPL Analysis\").getOrCreate()\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c289f2e0-0f7b-4903-bd86-ad66a2b67550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [season_id: int, season_label: string ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[season_id: int, season_label: string ... 8 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//import transformed data\n",
    "val df = spark.read\n",
    "  .format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"data/data_for_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae116edd-23d1-4ecc-b36f-4ff74d7e39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "//to partquet for editing\n",
    "df.write\n",
    "  .mode(\"overwrite\")\n",
    "  .parquet(\"data/model_master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f98e170-a4c1-4c6b-b0d0-1116e517a47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "currentTable = [team: string, pts: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+---+---+\n",
      "|team              |pts|gf |ga |\n",
      "+------------------+---+---+---+\n",
      "|Lion City Sailors |15 |22 |2  |\n",
      "|Geylang Int.      |10 |7  |4  |\n",
      "|Tampines Rovers   |8  |8  |4  |\n",
      "|Balestier Khalsa  |8  |9  |12 |\n",
      "|Albirex Niigata   |7  |8  |5  |\n",
      "|Hougang Utd       |3  |4  |10 |\n",
      "|Tanjong Pagar Utd.|3  |7  |15 |\n",
      "|Young Lions       |0  |4  |17 |\n",
      "+------------------+---+---+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[team: string, pts: int ... 2 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//current league table\n",
    "//include goals for, goals against\n",
    "val currentTable = Seq(\n",
    "  (\"Lion City Sailors\", 15, 22, 2),\n",
    "  (\"Geylang Int.\", 10, 7, 4),\n",
    "  (\"Tampines Rovers\", 8, 8, 4),\n",
    "  (\"Balestier Khalsa\", 8, 9, 12),\n",
    "  (\"Albirex Niigata\", 7, 8, 5),\n",
    "  (\"Hougang Utd\", 3, 4, 10),\n",
    "  (\"Tanjong Pagar Utd.\", 3, 7, 15),\n",
    "  (\"Young Lions\", 0, 4, 17)\n",
    ").toDF(\"team\", \"pts\", \"gf\", \"ga\")\n",
    "currentTable.show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d3c45f-9f6b-48e9-b516-b0eb43a2084e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eventSchema = StructType(StructField(event_id,LongType,true),StructField(season_year,StringType,true),StructField(home_team,StringType,true),StructField(away_team,StringType,true),StructField(home_score,LongType,true),StructField(away_score,LongType,true),StructField(status_code,LongType,true),StructField(start_timestamp,LongType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(event_id,LongType,true),StructField(season_year,StringType,true),StructField(home_team,StringType,true),StructField(away_team,StringType,true),StructField(home_score,LongType,true),StructField(away_score,LongType,true),StructField(status_code,LongType,true),StructField(start_timestamp,LongType,true))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//stream to update current table\n",
    "// NOTE: Avoid case class encoders in notebooks/REPL (can cause \"Unable to generate an encoder for inner class...\").\n",
    "// Use an explicit StructType schema instead.\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val eventSchema = StructType(Seq(\n",
    "  StructField(\"event_id\", LongType, nullable = true),\n",
    "  StructField(\"season_year\", StringType, nullable = true),\n",
    "  StructField(\"home_team\", StringType, nullable = true),\n",
    "  StructField(\"away_team\", StringType, nullable = true),\n",
    "  StructField(\"home_score\", LongType, nullable = true),\n",
    "  StructField(\"away_score\", LongType, nullable = true),\n",
    "  StructField(\"status_code\", LongType, nullable = true),\n",
    "  StructField(\"start_timestamp\", LongType, nullable = true)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9e76310-b858-4d56-8ce5-2be42f3d7c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eventStream = [event_id: bigint, season_year: string ... 7 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[event_id: bigint, season_year: string ... 7 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val eventStream = spark.readStream\n",
    "  .schema(eventSchema)\n",
    "  .option(\"multiLine\", \"true\")\n",
    "  .json(\"data/stream_in/event\")\n",
    "  .withColumn(\"source_file\", input_file_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "638ca077-15b4-4ded-8752-e55b52cab679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "perMatchTeamDelta = [event_id: bigint, source_file: string ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[event_id: bigint, source_file: string ... 4 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//one match will affect 2 teams on the table\n",
    "val perMatchTeamDelta =\n",
    "  eventStream\n",
    "    // Guard against wrong JSON being dropped into the event folder (e.g., stats JSON).\n",
    "    .filter(\n",
    "      col(\"status_code\") === 100 &&\n",
    "      col(\"home_team\").isNotNull && col(\"away_team\").isNotNull &&\n",
    "      col(\"home_score\").isNotNull && col(\"away_score\").isNotNull\n",
    "    )\n",
    "//get pts for home team\n",
    "    .withColumn(\n",
    "      \"home_pts\",\n",
    "      when(col(\"home_score\") > col(\"away_score\"), lit(3))\n",
    "        .when(col(\"home_score\") === col(\"away_score\"), lit(1))\n",
    "        .otherwise(lit(0))\n",
    "    )\n",
    "//get pts for away team\n",
    "    .withColumn(\n",
    "      \"away_pts\",\n",
    "      when(col(\"away_score\") > col(\"home_score\"), lit(3))\n",
    "        .when(col(\"away_score\") === col(\"home_score\"), lit(1))\n",
    "        .otherwise(lit(0))\n",
    "    )\n",
    "    .select(\n",
    "      col(\"event_id\"),\n",
    "      col(\"source_file\"),\n",
    "      array(\n",
    "        struct(\n",
    "          col(\"home_team\").alias(\"team\"),\n",
    "          col(\"home_pts\").alias(\"pts_delta\"),\n",
    "          col(\"home_score\").alias(\"gf_delta\"),\n",
    "          col(\"away_score\").alias(\"ga_delta\")\n",
    "        ),\n",
    "        struct(\n",
    "          col(\"away_team\").alias(\"team\"),\n",
    "          col(\"away_pts\").alias(\"pts_delta\"),\n",
    "          col(\"away_score\").alias(\"gf_delta\"),\n",
    "          col(\"home_score\").alias(\"ga_delta\")\n",
    "        )\n",
    "      ).alias(\"rows\")\n",
    "    )\n",
    "    .withColumn(\"row\", explode(col(\"rows\")))\n",
    "    .select(\n",
    "      col(\"event_id\"),\n",
    "      col(\"source_file\"),\n",
    "      col(\"row.team\").alias(\"team\"),\n",
    "      col(\"row.pts_delta\").alias(\"pts_delta\"),\n",
    "      col(\"row.gf_delta\").alias(\"gf_delta\"),\n",
    "      col(\"row.ga_delta\").alias(\"ga_delta\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17a48535-ee96-445f-8a7e-decc5fdca854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aggregateDelta: (perMatchTeamDeltaBatch: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Aggregate changes (batch-level).\n",
    "// NOTE: We do this inside foreachBatch so we don't need streaming state/checkpoints.\n",
    "def aggregateDelta(perMatchTeamDeltaBatch: DataFrame): DataFrame = {\n",
    "  perMatchTeamDeltaBatch\n",
    "    .groupBy(col(\"team\"))\n",
    "    .agg(\n",
    "      sum(col(\"pts_delta\")).alias(\"pts_add\"),\n",
    "      sum(col(\"gf_delta\")).alias(\"gf_add\"),\n",
    "      sum(col(\"ga_delta\")).alias(\"ga_add\")\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abbcbe53-38e5-40df-811f-d5ebcda4ae5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1bd92154\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@1bd92154"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[EVENT] batchId=0 | inputRows=4\n",
      "[EVENT] files:\n",
      "+------------------------------------------------------------------------------+\n",
      "|source_file                                                                   |\n",
      "+------------------------------------------------------------------------------+\n",
      "|file:///workspace/PowerRanking/data/stream_in/event/event_14195502%20copy.json|\n",
      "|file:///workspace/PowerRanking/data/stream_in/event/event_14195502.json       |\n",
      "+------------------------------------------------------------------------------+\n",
      "\n",
      "[EVENT] deltaTeams=2\n",
      "=== batchId=0 ===\n",
      "+------------------+---+---+---+\n",
      "|team              |pts|gf |ga |\n",
      "+------------------+---+---+---+\n",
      "|Lion City Sailors |15 |22 |2  |\n",
      "|Balestier Khalsa  |14 |13 |14 |\n",
      "|Geylang Int.      |10 |7  |4  |\n",
      "|Tampines Rovers   |8  |8  |4  |\n",
      "|Albirex Niigata   |7  |8  |5  |\n",
      "|Hougang Utd       |3  |4  |10 |\n",
      "|Tanjong Pagar Utd.|3  |9  |19 |\n",
      "|Young Lions       |0  |4  |17 |\n",
      "+------------------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// NO CHECKPOINTING (demo mode): do all aggregation/update per micro-batch.\n",
    "val q = perMatchTeamDelta.writeStream\n",
    "  .outputMode(\"append\")\n",
    "  .foreachBatch { (batchRows: DataFrame, batchId: Long) =>\n",
    "\n",
    "    println(s\"[EVENT] batchId=$batchId | inputRows=${batchRows.count()}\")\n",
    "    println(\"[EVENT] files:\")\n",
    "    batchRows.select(\"source_file\").distinct().show(50, truncate = false)\n",
    "\n",
    "    // If someone drops stats JSON into event folder, it will be filtered out in perMatchTeamDelta.\n",
    "    val batchDelta = aggregateDelta(batchRows)\n",
    "    println(s\"[EVENT] deltaTeams=${batchDelta.count()}\")\n",
    "\n",
    "    val baseTable =\n",
    "      if (spark.catalog.tableExists(\"current_table_live\")) spark.table(\"current_table_live\")\n",
    "      else currentTable\n",
    "\n",
    "    val updated =\n",
    "      baseTable.as(\"c\")\n",
    "        .join(batchDelta.as(\"d\"), Seq(\"team\"), \"left\")\n",
    "        .select(\n",
    "          col(\"team\"),\n",
    "          (col(\"c.pts\") + coalesce(col(\"d.pts_add\"), lit(0))).alias(\"pts\"),\n",
    "          (col(\"c.gf\")  + coalesce(col(\"d.gf_add\"),  lit(0))).alias(\"gf\"),\n",
    "          (col(\"c.ga\")  + coalesce(col(\"d.ga_add\"),  lit(0))).alias(\"ga\")\n",
    "        )\n",
    "        .orderBy(desc(\"pts\"), (col(\"gf\") - col(\"ga\")).desc, desc(\"gf\"))\n",
    "\n",
    "    updated.createOrReplaceTempView(\"current_table_live\")\n",
    "    println(s\"=== batchId=$batchId ===\")\n",
    "    updated.show(50, truncate = false)\n",
    "  }\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c2086d3-8197-4fdb-8067-528396621e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "statsSchema = StructType(StructField(event_id,LongType,true),StructField(home_xg,DoubleType,true),StructField(away_xg,DoubleType,true))\n",
       "statsStream = [event_id: bigint, home_xg: double ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[event_id: bigint, home_xg: double ... 2 more fields]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//stats streaming\n",
    "// NOTE: Avoid case class encoders in notebooks/REPL; use explicit schema.\n",
    "val statsSchema = StructType(Seq(\n",
    "  StructField(\"event_id\", LongType, nullable = true),\n",
    "  StructField(\"home_xg\", DoubleType, nullable = true),\n",
    "  StructField(\"away_xg\", DoubleType, nullable = true)\n",
    "))\n",
    "\n",
    "val statsStream = spark.readStream\n",
    "  .schema(statsSchema)\n",
    "  .option(\"multiLine\", \"true\")\n",
    "  .option(\"ignoreMissingFiles\", \"true\")\n",
    "  .option(\"ignoreCorruptFiles\", \"true\")\n",
    "  .json(\"data/stream_in/stats\")\n",
    "  .withColumn(\"source_file\", input_file_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4029557b-9d27-4209-8272-6251e20fea54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "upsertStatsIntoMaster: (statsBatch: org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "def upsertStatsIntoMaster(statsBatch: DataFrame): Unit = {\n",
    "\n",
    "  val masterPath = \"data/model_master\"\n",
    "\n",
    "  // Initialize master parquet if it doesn't exist yet (seed from the base CSV df).\n",
    "  // In this project we use a parquet folder at `data/model_master` as the streaming-updated master.\n",
    "  val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)\n",
    "  val p = new Path(masterPath)\n",
    "\n",
    "  val exists = fs.exists(p)\n",
    "  val empty = exists && {\n",
    "    val st = fs.listStatus(p)\n",
    "    st == null || st.isEmpty\n",
    "  }\n",
    "\n",
    "  if (!exists || empty) {\n",
    "    if (empty) {\n",
    "      fs.delete(p, true)\n",
    "      println(s\"[INIT] $masterPath existed but was empty -> recreating from df\")\n",
    "    } else {\n",
    "      println(s\"[INIT] $masterPath not found -> creating from df (data_for_model.csv)\")\n",
    "    }\n",
    "    df.write.mode(\"overwrite\").parquet(masterPath)\n",
    "  }\n",
    "\n",
    "  // Rename incoming columns to avoid duplicate column names after join.\n",
    "  // We only want to fill nulls in the master (not overwrite existing xG).\n",
    "  val updates = statsBatch\n",
    "    .select(\n",
    "      col(\"event_id\"),\n",
    "      col(\"home_xg\").cast(\"double\").alias(\"u_home_expected_goals\"),\n",
    "      col(\"away_xg\").cast(\"double\").alias(\"u_away_expected_goals\")\n",
    "    )\n",
    "    .dropDuplicates(\"event_id\")\n",
    "\n",
    "  val master = spark.read.parquet(masterPath)\n",
    "\n",
    "  // Debug: verify whether incoming event_ids actually match master.\n",
    "  val incomingUnique = updates.select(\"event_id\").count()\n",
    "  val matchedInMaster =\n",
    "    master.select(\"event_id\").dropDuplicates()\n",
    "      .join(updates.select(\"event_id\"), Seq(\"event_id\"), \"inner\")\n",
    "      .count()\n",
    "  val labeledBefore = master\n",
    "    .filter(col(\"home_expected_goals\").isNotNull && col(\"away_expected_goals\").isNotNull)\n",
    "    .count()\n",
    "\n",
    "  println(s\"[DATA] stats incoming unique=$incomingUnique | matched_in_master=$matchedInMaster | unmatched=${incomingUnique - matchedInMaster} | labeled_rows_before=$labeledBefore\")\n",
    "\n",
    "  val updated =\n",
    "    master.as(\"m\")\n",
    "      .join(updates.as(\"u\"), Seq(\"event_id\"), \"left\")\n",
    "      .withColumn(\n",
    "        \"home_expected_goals\",\n",
    "        coalesce(col(\"m.home_expected_goals\"), col(\"u.u_home_expected_goals\"))\n",
    "      )\n",
    "      .withColumn(\n",
    "        \"away_expected_goals\",\n",
    "        coalesce(col(\"m.away_expected_goals\"), col(\"u.u_away_expected_goals\"))\n",
    "      )\n",
    "      .withColumn(\n",
    "        \"is_future_fixture\",\n",
    "        when(col(\"u.event_id\").isNotNull, lit(0)) // stats arrived â†’ no longer future\n",
    "          .otherwise(col(\"m.is_future_fixture\"))\n",
    "      )\n",
    "      .drop(\"u_home_expected_goals\", \"u_away_expected_goals\")\n",
    "\n",
    "  updated.write.mode(\"overwrite\").parquet(masterPath)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8039a9c-d1d5-4392-ab4c-26bd1f4840ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AUTO] bootstrapping dashboard from df/master -> refreshDashboard()\n",
      "[AUTO] dashboard_live (top 10)\n",
      "+------------------+----+---+---+--------------------+----------------+--------------+------------+------------------+\n",
      "|team              |pts |gf |ga |exp_pts_remaining_mc|exp_pts_final_mc|win_league_pct|make_acl_pct|spi               |\n",
      "+------------------+----+---+---+--------------------+----------------+--------------+------------+------------------+\n",
      "|Lion City Sailors |15.0|22 |2  |37.7883             |52.7883         |96.64         |99.89       |89.47321462247646 |\n",
      "|Tampines Rovers   |8.0 |8  |4  |31.90965            |39.90965        |3.145         |76.33       |88.7796109400253  |\n",
      "|Albirex Niigata   |7.0 |8  |5  |27.139200000000002  |34.1392         |0.2           |20.21       |82.79783898992036 |\n",
      "|Geylang Int.      |10.0|7  |4  |17.8014             |27.8014         |0.005         |2.045       |74.22054475013204 |\n",
      "|Balestier Khalsa  |8.0 |9  |12 |18.26305            |26.26305        |0.01          |1.525       |76.38412017949582 |\n",
      "|Hougang Utd       |3.0 |4  |10 |13.7406             |16.7406         |0.0           |0.0         |72.14284724583914 |\n",
      "|Tanjong Pagar Utd.|3.0 |7  |15 |11.3172             |14.3172         |0.0           |0.0         |59.37554617748957 |\n",
      "|Young Lions       |0.0 |4  |17 |8.3676              |8.3676          |0.0           |0.0         |62.914669143373864|\n",
      "+------------------+----+---+---+--------------------+----------------+--------------+------------+------------------+\n",
      "\n",
      "[AUTO] spi_live (top 10)\n",
      "+------------------+------------------+\n",
      "|team              |spi               |\n",
      "+------------------+------------------+\n",
      "|Lion City Sailors |89.47321462247646 |\n",
      "|Tampines Rovers   |88.7796109400253  |\n",
      "|Albirex Niigata   |82.79783898992036 |\n",
      "|Balestier Khalsa  |76.38412017949582 |\n",
      "|UNKNOWN           |75.15286425551882 |\n",
      "|Geylang Int.      |74.22054475013204 |\n",
      "|Hougang Utd       |72.14284724583914 |\n",
      "|DPMM              |68.75874369572863 |\n",
      "|Young Lions       |62.914669143373864|\n",
      "|Tanjong Pagar Utd.|59.37554617748957 |\n",
      "+------------------+------------------+\n",
      "\n",
      "[AUTO] fixtures_live (top 10)\n",
      "+--------+----------+------------------+------------------+-----+------+----------+-----+------+------+-------+\n",
      "|event_id|match_date|team              |opponent          |venue|xg_for|xg_against|p_win|p_draw|p_loss|exp_pts|\n",
      "+--------+----------+------------------+------------------+-----+------+----------+-----+------+------+-------+\n",
      "|14195506|2026-01-09|Geylang Int.      |Balestier Khalsa  |H    |1.82  |2.28      |31.42|19.99 |48.59 |1.14   |\n",
      "|14195506|2026-01-09|Balestier Khalsa  |Geylang Int.      |A    |2.28  |1.82      |48.59|19.99 |31.42 |1.66   |\n",
      "|14195507|2026-01-10|Albirex Niigata   |Young Lions       |H    |2.67  |0.76      |77.92|13.97 |8.11  |2.48   |\n",
      "|14195507|2026-01-10|Young Lions       |Albirex Niigata   |A    |0.76  |2.67      |8.11 |13.97 |77.92 |0.38   |\n",
      "|14195508|2026-01-11|Tampines Rovers   |Lion City Sailors |H    |1.69  |1.8       |36.61|22.26 |41.13 |1.32   |\n",
      "|14195508|2026-01-11|Lion City Sailors |Tampines Rovers   |A    |1.8   |1.69      |41.13|22.26 |36.61 |1.46   |\n",
      "|14195509|2026-01-12|Tanjong Pagar Utd.|Hougang Utd       |H    |1.57  |1.71      |35.4 |23.06 |41.55 |1.29   |\n",
      "|14195509|2026-01-12|Hougang Utd       |Tanjong Pagar Utd.|A    |1.71  |1.57      |41.55|23.06 |35.4  |1.48   |\n",
      "|14195510|2026-01-16|Geylang Int.      |Hougang Utd       |H    |1.68  |1.4       |44.37|23.67 |31.96 |1.57   |\n",
      "|14195510|2026-01-16|Hougang Utd       |Geylang Int.      |A    |1.4   |1.68      |31.96|23.67 |44.37 |1.2    |\n",
      "+--------+----------+------------------+------------------+-----+------+----------+-----+------+------+-------+\n",
      "only showing top 10 rows\n",
      "\n",
      "[AUTO] bootstrap done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MASTER_PATH = data/model_master\n",
       "MODEL_HOME_PATH = data/models/glm_home_pipeline_latest\n",
       "MODEL_AWAY_PATH = data/models/glm_away_pipeline_latest\n",
       "DASH_OUT = data/dashboard_table_csv\n",
       "FIXT_OUT = data/match_level_fixtures\n",
       "DEFAULT_MC_SIMS = 20000\n",
       "dashboardBootstrapped = true\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: four deprecations (since 3.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
       "refitHomeAwayPipelines: (trainRaw: org.apache.spark.sql.DataFrame)(org.apache.spark.ml.PipelineModel, org.apache.spark.ml.PipelineModel)\n",
       "ensureMasterParquet: ()Unit\n",
       "tryLoadPipeline: (path: String)Option[org.apache.spark.ml.PipelineModel]\n",
       "loadOrTrainPipelines: ()(org.apache.spark.m...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//pipeline refit function for home + away GLM\n",
    "def refitHomeAwayPipelines(trainRaw: DataFrame): (PipelineModel, PipelineModel) = {\n",
    "  // IMPORTANT: GLM label columns are home_xg / away_xg\n",
    "  // but the master parquet stores them as home_expected_goals / away_expected_goals.\n",
    "  val train = trainRaw.select(\n",
    "    col(\"home_team\"),\n",
    "    col(\"away_team\"),\n",
    "    col(\"home_expected_goals\").cast(\"double\").alias(\"home_xg\"),\n",
    "    col(\"away_expected_goals\").cast(\"double\").alias(\"away_xg\"),\n",
    "    lit(1.0).alias(\"home_adv\")\n",
    "  )\n",
    "\n",
    "  val home_idx = new StringIndexer()\n",
    "    .setInputCol(\"home_team\")\n",
    "    .setOutputCol(\"home_team_idx\")\n",
    "    .setHandleInvalid(\"keep\")\n",
    "\n",
    "  val away_idx = new StringIndexer()\n",
    "    .setInputCol(\"away_team\")\n",
    "    .setOutputCol(\"away_team_idx\")\n",
    "    .setHandleInvalid(\"keep\")\n",
    "\n",
    "  val enc = new OneHotEncoder()\n",
    "    .setInputCols(Array(\"home_team_idx\", \"away_team_idx\"))\n",
    "    .setOutputCols(Array(\"home_team_ohe\", \"away_team_ohe\"))\n",
    "\n",
    "  val assembler = new VectorAssembler()\n",
    "    .setInputCols(Array(\"home_team_ohe\", \"away_team_ohe\", \"home_adv\"))\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "  val glm_home = new GeneralizedLinearRegression()\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setLabelCol(\"home_xg\")\n",
    "    .setFamily(\"gaussian\")\n",
    "    .setLink(\"identity\")\n",
    "    .setMaxIter(50)\n",
    "    .setRegParam(0.0)\n",
    "\n",
    "  val glm_away = new GeneralizedLinearRegression()\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setLabelCol(\"away_xg\")\n",
    "    .setFamily(\"gaussian\")\n",
    "    .setLink(\"identity\")\n",
    "    .setMaxIter(50)\n",
    "    .setRegParam(0.0)\n",
    "\n",
    "  val pipe_home = new Pipeline().setStages(Array(home_idx, away_idx, enc, assembler, glm_home))\n",
    "  val pipe_away = new Pipeline().setStages(Array(home_idx, away_idx, enc, assembler, glm_away))\n",
    "\n",
    "  val model_home = pipe_home.fit(train)\n",
    "  val model_away = pipe_away.fit(train)\n",
    "\n",
    "  (model_home, model_away)\n",
    "}\n",
    "\n",
    "// -----------------------------\n",
    "// Pattern B helpers: load-or-train + refresh exports\n",
    "// -----------------------------\n",
    "// (PipelineModel is imported in Cell 0)\n",
    "import org.apache.spark.ml.feature.{StringIndexerModel, OneHotEncoderModel}\n",
    "import org.apache.spark.ml.regression.GeneralizedLinearRegressionModel\n",
    "\n",
    "val MASTER_PATH = \"data/model_master\"\n",
    "val MODEL_HOME_PATH = \"data/models/glm_home_pipeline_latest\"\n",
    "val MODEL_AWAY_PATH = \"data/models/glm_away_pipeline_latest\"\n",
    "val DASH_OUT = \"data/dashboard_table_csv\"\n",
    "val FIXT_OUT = \"data/match_level_fixtures\"\n",
    "\n",
    "// Keep these small-ish for \"live\" refresh. You can bump them if runtime is OK.\n",
    "val DEFAULT_MC_SIMS = 20000\n",
    "\n",
    "@volatile var dashboardBootstrapped: Boolean = false\n",
    "\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "def ensureMasterParquet(): Unit = {\n",
    "  val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)\n",
    "  val p = new Path(MASTER_PATH)\n",
    "  if (!fs.exists(p)) {\n",
    "    println(s\"[INIT] $MASTER_PATH not found -> creating from df (data_for_model.csv)\")\n",
    "    df.write.mode(\"overwrite\").parquet(MASTER_PATH)\n",
    "  }\n",
    "}\n",
    "\n",
    "def tryLoadPipeline(path: String): Option[PipelineModel] = {\n",
    "  try Some(PipelineModel.load(path))\n",
    "  catch { case _: Throwable => None }\n",
    "}\n",
    "\n",
    "def loadOrTrainPipelines(): (PipelineModel, PipelineModel) = {\n",
    "  val mh = tryLoadPipeline(MODEL_HOME_PATH)\n",
    "  val ma = tryLoadPipeline(MODEL_AWAY_PATH)\n",
    "  if (mh.isDefined && ma.isDefined) return (mh.get, ma.get)\n",
    "\n",
    "  ensureMasterParquet()\n",
    "  val master = spark.read.parquet(MASTER_PATH)\n",
    "  val modelDF = master\n",
    "    .filter(col(\"home_expected_goals\").isNotNull && col(\"away_expected_goals\").isNotNull)\n",
    "    .dropDuplicates(\"event_id\")\n",
    "\n",
    "  val (h, a) = refitHomeAwayPipelines(modelDF)\n",
    "  h.write.overwrite().save(MODEL_HOME_PATH)\n",
    "  a.write.overwrite().save(MODEL_AWAY_PATH)\n",
    "  (h, a)\n",
    "}\n",
    "\n",
    "def getLiveTableSafe(): DataFrame = {\n",
    "  if (spark.catalog.tableExists(\"current_table_live\")) spark.table(\"current_table_live\")\n",
    "  else currentTable\n",
    "}\n",
    "\n",
    "def powerRankFromHomePipeline(modelHome: PipelineModel): DataFrame = {\n",
    "  // Avoid Seq(...).toDF + zip/:+ patterns (Scala REPL can get unstable after re-runs).\n",
    "  import org.apache.spark.sql.Row\n",
    "  import org.apache.spark.sql.types._\n",
    "\n",
    "  val stages = modelHome.stages\n",
    "  val homeIdx = stages(0).asInstanceOf[StringIndexerModel]\n",
    "  val awayIdx = stages(1).asInstanceOf[StringIndexerModel]\n",
    "  val ohe = stages(2).asInstanceOf[OneHotEncoderModel]\n",
    "  val glm = stages(4).asInstanceOf[GeneralizedLinearRegressionModel]\n",
    "\n",
    "  val coeff = glm.coefficients.toArray\n",
    "  val homeCats = ohe.categorySizes(0)\n",
    "  val awayCats = ohe.categorySizes(1)\n",
    "  val dropLast = ohe.getDropLast\n",
    "  val homeDim = if (dropLast) homeCats - 1 else homeCats\n",
    "  val awayDim = if (dropLast) awayCats - 1 else awayCats\n",
    "\n",
    "  val homeAttackCoef = coeff.slice(0, homeDim)\n",
    "  val awayDefenseCoef = coeff.slice(homeDim, homeDim + awayDim)\n",
    "\n",
    "  val homeTeams = if (homeIdx.getHandleInvalid == \"keep\") homeIdx.labels :+ \"UNKNOWN\" else homeIdx.labels\n",
    "  val awayTeams = if (awayIdx.getHandleInvalid == \"keep\") awayIdx.labels :+ \"UNKNOWN\" else awayIdx.labels\n",
    "\n",
    "  def attachBaselineRows(teams: Array[String], coef: Array[Double]): Seq[Row] = {\n",
    "    val out = new Array[Row](teams.length)\n",
    "    var i = 0\n",
    "    while (i < teams.length) {\n",
    "      val v = if (i < coef.length) coef(i) else 0.0\n",
    "      out(i) = Row(teams(i), v)\n",
    "      i += 1\n",
    "    }\n",
    "    out.toSeq\n",
    "  }\n",
    "\n",
    "  def pairDF(rows: Seq[Row], valueCol: String): DataFrame = {\n",
    "    val schema = StructType(Seq(\n",
    "      StructField(\"team\", StringType, nullable = false),\n",
    "      StructField(valueCol, DoubleType, nullable = false)\n",
    "    ))\n",
    "    spark.createDataFrame(spark.sparkContext.parallelize(rows), schema)\n",
    "  }\n",
    "\n",
    "  val homeAttack = pairDF(attachBaselineRows(homeTeams, homeAttackCoef), \"home_attack\")\n",
    "  val awayDefWeak = pairDF(attachBaselineRows(awayTeams, awayDefenseCoef), \"away_def_weak\")\n",
    "\n",
    "  homeAttack\n",
    "    .join(awayDefWeak, \"team\")\n",
    "    .withColumn(\"power_score\", col(\"home_attack\") - col(\"away_def_weak\"))\n",
    "    .orderBy(desc(\"power_score\"))\n",
    "}\n",
    "\n",
    "// Full refresh: score fixtures -> compute odds/MC/SPI -> export CSVs for Streamlit\n",
    "// This is what makes Pattern B non-redundant: predictions always come from the latest saved pipeline models.\n",
    "def refreshDashboard(seasonLabel: String = \"25/26\", mcSims: Int = DEFAULT_MC_SIMS): Unit = {\n",
    "  // Avoid relying on spark.implicits / .toDF in notebooks (REPL can get unstable after re-runs).\n",
    "  import org.apache.spark.sql.Row\n",
    "  import org.apache.spark.sql.types._\n",
    "\n",
    "  val liveTable = getLiveTableSafe().cache()\n",
    "\n",
    "  val (modelHome, modelAway) = loadOrTrainPipelines()\n",
    "  ensureMasterParquet()\n",
    "  val master = spark.read.parquet(MASTER_PATH)\n",
    "\n",
    "  // 1) Score future fixtures\n",
    "  val fixtures = master\n",
    "    .filter(col(\"is_future_fixture\") === 1 && col(\"season_label\") === seasonLabel)\n",
    "    .select(\n",
    "      col(\"event_id\"),\n",
    "      col(\"match_date\"),\n",
    "      trim(col(\"home_team\")).alias(\"home_team\"),\n",
    "      trim(col(\"away_team\")).alias(\"away_team\"),\n",
    "      lit(1.0).alias(\"home_adv\")\n",
    "    )\n",
    "    .dropDuplicates(\"event_id\")\n",
    "\n",
    "  // IMPORTANT: modelHome and modelAway are both Pipelines that create the same intermediate columns\n",
    "  // (home_team_idx, away_team_idx, *ohe, features). If we feed the full output of modelHome into modelAway,\n",
    "  // the second transform fails with: \"Output column home_team_idx already exists\".\n",
    "  val predHomeFull = modelHome.transform(fixtures)\n",
    "    .withColumnRenamed(\"prediction\", \"lambda_home\")\n",
    "\n",
    "  // Drop intermediate pipeline columns by selecting only what modelAway needs.\n",
    "  val predHome = predHomeFull.select(\n",
    "    col(\"event_id\"),\n",
    "    col(\"match_date\"),\n",
    "    col(\"home_team\"),\n",
    "    col(\"away_team\"),\n",
    "    col(\"home_adv\"),\n",
    "    col(\"lambda_home\")\n",
    "  )\n",
    "\n",
    "  val predBothFull = modelAway.transform(predHome)\n",
    "    .withColumnRenamed(\"prediction\", \"lambda_away\")\n",
    "\n",
    "  val predBoth = predBothFull.select(\n",
    "    col(\"event_id\"),\n",
    "    col(\"match_date\"),\n",
    "    col(\"home_team\"),\n",
    "    col(\"away_team\"),\n",
    "    col(\"lambda_home\"),\n",
    "    col(\"lambda_away\")\n",
    "  )\n",
    "\n",
    "  // 2) W/D/L probabilities (local UDF to avoid global symbol clashes)\n",
    "  import scala.math.{exp, pow}\n",
    "\n",
    "  def poissonPmf(k: Int, lambda: Double): Double = {\n",
    "    if (k < 0) 0.0\n",
    "    else {\n",
    "      var fact = 1.0\n",
    "      var i = 2\n",
    "      while (i <= k) { fact *= i; i += 1 }\n",
    "      exp(-lambda) * pow(lambda, k) / fact\n",
    "    }\n",
    "  }\n",
    "\n",
    "  case class WDL(win: Double, draw: Double, loss: Double)\n",
    "\n",
    "  def wdlFromLambdas(lhRaw: Double, laRaw: Double, maxGoals: Int = 10): WDL = {\n",
    "    val lh = math.max(lhRaw, 0.0)\n",
    "    val la = math.max(laRaw, 0.0)\n",
    "\n",
    "    val ph = Array.tabulate(maxGoals + 1)(k => poissonPmf(k, lh))\n",
    "    val pa = Array.tabulate(maxGoals + 1)(k => poissonPmf(k, la))\n",
    "\n",
    "    var mass = 0.0\n",
    "    var i = 0\n",
    "    while (i <= maxGoals) {\n",
    "      var j = 0\n",
    "      while (j <= maxGoals) { mass += ph(i) * pa(j); j += 1 }\n",
    "      i += 1\n",
    "    }\n",
    "    val denom = if (mass > 0) mass else 1.0\n",
    "\n",
    "    var win = 0.0\n",
    "    var draw = 0.0\n",
    "    i = 0\n",
    "    while (i <= maxGoals) {\n",
    "      var j = 0\n",
    "      while (j <= maxGoals) {\n",
    "        val p = (ph(i) * pa(j)) / denom\n",
    "        if (i > j) win += p\n",
    "        else if (i == j) draw += p\n",
    "        j += 1\n",
    "      }\n",
    "      i += 1\n",
    "    }\n",
    "    WDL(win, draw, 1.0 - win - draw)\n",
    "  }\n",
    "\n",
    "  val wdlUdfLocal = udf((lh: Double, la: Double) => {\n",
    "    val r = wdlFromLambdas(lh, la, maxGoals = 10)\n",
    "    (r.win, r.draw, r.loss)\n",
    "  })\n",
    "\n",
    "  val matchProbs =\n",
    "    predBoth\n",
    "      .withColumn(\"wdl\", wdlUdfLocal(col(\"lambda_home\"), col(\"lambda_away\")))\n",
    "      .withColumn(\"p_home_win\", col(\"wdl\").getField(\"_1\"))\n",
    "      .withColumn(\"p_draw\",     col(\"wdl\").getField(\"_2\"))\n",
    "      .withColumn(\"p_away_win\", col(\"wdl\").getField(\"_3\"))\n",
    "      .drop(\"wdl\")\n",
    "      .withColumn(\"exp_pts_home\", lit(3.0) * col(\"p_home_win\") + lit(1.0) * col(\"p_draw\"))\n",
    "      .withColumn(\"exp_pts_away\", lit(3.0) * col(\"p_away_win\") + lit(1.0) * col(\"p_draw\"))\n",
    "      .withColumnRenamed(\"lambda_home\", \"xg_home\")\n",
    "      .withColumnRenamed(\"lambda_away\", \"xg_away\")\n",
    "\n",
    "  // 3) Monte Carlo season sims (using predicted lambdas)\n",
    "  import scala.util.Random\n",
    "\n",
    "  case class TeamState(var pts: Int, var gf: Int, var ga: Int)\n",
    "\n",
    "  def poisson(lambdaRaw: Double, rng: Random): Int = {\n",
    "    val lambda = math.max(lambdaRaw, 0.0)\n",
    "    val L = math.exp(-lambda)\n",
    "    var k = 0\n",
    "    var p = 1.0\n",
    "    do {\n",
    "      k += 1\n",
    "      p *= rng.nextDouble()\n",
    "    } while (p > L && k < 25)\n",
    "    k - 1\n",
    "  }\n",
    "\n",
    "  def goalDiff(s: TeamState): Int = s.gf - s.ga\n",
    "\n",
    "  def simulateSeasonOnce(\n",
    "    teams: Array[String],\n",
    "    base: Map[String, TeamState],\n",
    "    matches: Array[MatchSim],\n",
    "    seed: Int\n",
    "  ): Map[String, TeamState] = {\n",
    "\n",
    "    val rng = new Random(seed)\n",
    "\n",
    "    val st = scala.collection.mutable.Map[String, TeamState]()\n",
    "    teams.foreach { t =>\n",
    "      val b = base(t)\n",
    "      st(t) = TeamState(b.pts, b.gf, b.ga)\n",
    "    }\n",
    "\n",
    "    matches.foreach { m =>\n",
    "      val gh = poisson(m.lamH, rng)\n",
    "      val ga = poisson(m.lamA, rng)\n",
    "\n",
    "      st(m.h).gf += gh; st(m.h).ga += ga\n",
    "      st(m.a).gf += ga; st(m.a).ga += gh\n",
    "\n",
    "      if (gh > ga) st(m.h).pts += 3\n",
    "      else if (gh < ga) st(m.a).pts += 3\n",
    "      else { st(m.h).pts += 1; st(m.a).pts += 1 }\n",
    "    }\n",
    "\n",
    "    st.toMap\n",
    "  }\n",
    "\n",
    "  val teams: Array[String] =\n",
    "    liveTable.select(\"team\").collect().map(r => r.getString(0))\n",
    "\n",
    "  val baseTable: Map[String, TeamState] =\n",
    "    liveTable.select(\"team\", \"pts\", \"gf\", \"ga\").collect()\n",
    "      .map { r =>\n",
    "        val t = r.getString(0)\n",
    "        val p = r.getAs[Number](1).longValue()\n",
    "        val gf = r.getAs[Number](2).longValue()\n",
    "        val ga = r.getAs[Number](3).longValue()\n",
    "        t -> TeamState(p.toInt, gf.toInt, ga.toInt)\n",
    "      }\n",
    "      .toMap\n",
    "\n",
    "  final case class MatchSim(h: String, a: String, lamH: Double, lamA: Double)\n",
    "\n",
    "  val matchesArr: Array[MatchSim] =\n",
    "    predBoth.select(\"home_team\", \"away_team\", \"lambda_home\", \"lambda_away\")\n",
    "      .collect()\n",
    "      .map { r =>\n",
    "        MatchSim(\n",
    "          r.getString(0),\n",
    "          r.getString(1),\n",
    "          r.getAs[Number](2).doubleValue(),\n",
    "          r.getAs[Number](3).doubleValue()\n",
    "        )\n",
    "      }\n",
    "\n",
    "  val winLeague = scala.collection.mutable.Map[String, Int]().withDefaultValue(0)\n",
    "  val makeACL   = scala.collection.mutable.Map[String, Int]().withDefaultValue(0)\n",
    "  val totalFinalPts = scala.collection.mutable.Map[String, Double]().withDefaultValue(0.0)\n",
    "\n",
    "  val N = math.max(mcSims, 1)\n",
    "\n",
    "  for (i <- 0 until N) {\n",
    "    val end = simulateSeasonOnce(teams, baseTable, matchesArr, seed = 1234 + i)\n",
    "\n",
    "    val ranked = teams.sortBy { t =>\n",
    "      val s = end(t)\n",
    "      (-s.pts, -goalDiff(s), -s.gf, t)\n",
    "    }\n",
    "\n",
    "    winLeague(ranked(0)) += 1\n",
    "    ranked.take(2).foreach(t => makeACL(t) += 1)\n",
    "    teams.foreach { t => totalFinalPts(t) += end(t).pts }\n",
    "  }\n",
    "\n",
    "  val mcPtsRows = teams.toSeq.map { t =>\n",
    "    val expFinal = totalFinalPts(t) / N\n",
    "    Row(t, expFinal)\n",
    "  }\n",
    "  val mcPtsSchema = StructType(Seq(\n",
    "    StructField(\"team\", StringType, nullable = false),\n",
    "    StructField(\"exp_pts_final_mc\", DoubleType, nullable = false)\n",
    "  ))\n",
    "  val mcPtsDF = spark.createDataFrame(spark.sparkContext.parallelize(mcPtsRows), mcPtsSchema)\n",
    "\n",
    "  val mcRemainingDF =\n",
    "    mcPtsDF\n",
    "      .join(liveTable.select($\"team\", $\"pts\"), Seq(\"team\"))\n",
    "      .withColumn(\"exp_pts_remaining_mc\", col(\"exp_pts_final_mc\") - col(\"pts\"))\n",
    "\n",
    "  val probsRows = teams.toSeq.map { t =>\n",
    "    val pWin = winLeague(t).toDouble / N\n",
    "    val pACL = makeACL(t).toDouble / N\n",
    "    Row(t, pWin * 100.0, pACL * 100.0)\n",
    "  }\n",
    "  val probsSchema = StructType(Seq(\n",
    "    StructField(\"team\", StringType, nullable = false),\n",
    "    StructField(\"win_league_pct\", DoubleType, nullable = false),\n",
    "    StructField(\"make_acl_pct\", DoubleType, nullable = false)\n",
    "  ))\n",
    "  val probsDF = spark.createDataFrame(spark.sparkContext.parallelize(probsRows), probsSchema)\n",
    "\n",
    "  // 4) SPI from power rank\n",
    "  val powerRank = powerRankFromHomePipeline(modelHome)\n",
    "  val w = Window.partitionBy()\n",
    "  val spiDF =\n",
    "    powerRank\n",
    "      .withColumn(\"mu\", avg($\"power_score\").over(w))\n",
    "      .withColumn(\"sd\", stddev($\"power_score\").over(w))\n",
    "      .withColumn(\"spi\", lit(75.0) + lit(10.0) * (($\"power_score\" - $\"mu\") / $\"sd\"))\n",
    "      .select(\"team\", \"spi\")\n",
    "\n",
    "  // Debug views (so you can show() them in separate cells)\n",
    "  powerRank.createOrReplaceTempView(\"power_rank_live\")\n",
    "  spiDF.createOrReplaceTempView(\"spi_live\")\n",
    "\n",
    "  // 5) Dashboard table export\n",
    "  val core =\n",
    "    liveTable\n",
    "      .select(\n",
    "        col(\"team\"),\n",
    "        col(\"pts\").cast(\"double\").alias(\"pts\"),\n",
    "        col(\"gf\").cast(\"int\").alias(\"gf\"),\n",
    "        col(\"ga\").cast(\"int\").alias(\"ga\")\n",
    "      )\n",
    "\n",
    "  val finalTable =\n",
    "    core\n",
    "      .join(mcRemainingDF.select(\"team\", \"exp_pts_remaining_mc\"), Seq(\"team\"), \"left\")\n",
    "      .join(mcPtsDF.select(\"team\", \"exp_pts_final_mc\"), Seq(\"team\"), \"left\")\n",
    "      .join(probsDF.select(\"team\", \"win_league_pct\", \"make_acl_pct\"), Seq(\"team\"), \"left\")\n",
    "      .join(spiDF.select(\"team\", \"spi\"), Seq(\"team\"), \"left\")\n",
    "      .na.fill(0.0, Seq(\"exp_pts_remaining_mc\", \"exp_pts_final_mc\", \"win_league_pct\", \"make_acl_pct\", \"spi\"))\n",
    "      .orderBy(desc(\"exp_pts_final_mc\"))\n",
    "\n",
    "  // Debug view\n",
    "  finalTable.createOrReplaceTempView(\"dashboard_live\")\n",
    "\n",
    "  finalTable\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(DASH_OUT)\n",
    "\n",
    "  // 6) Match-level fixtures export (team/opponent rows)\n",
    "  val homeView =\n",
    "    matchProbs.select(\n",
    "      col(\"event_id\"),\n",
    "      col(\"match_date\"),\n",
    "      col(\"home_team\").alias(\"team\"),\n",
    "      col(\"away_team\").alias(\"opponent\"),\n",
    "      lit(\"H\").alias(\"venue\"),\n",
    "      col(\"xg_home\").alias(\"xg_for\"),\n",
    "      col(\"xg_away\").alias(\"xg_against\"),\n",
    "      col(\"p_home_win\").alias(\"p_win\"),\n",
    "      col(\"p_draw\").alias(\"p_draw\"),\n",
    "      col(\"p_away_win\").alias(\"p_loss\"),\n",
    "      col(\"exp_pts_home\").alias(\"exp_pts\")\n",
    "    )\n",
    "\n",
    "  val awayView =\n",
    "    matchProbs.select(\n",
    "      col(\"event_id\"),\n",
    "      col(\"match_date\"),\n",
    "      col(\"away_team\").alias(\"team\"),\n",
    "      col(\"home_team\").alias(\"opponent\"),\n",
    "      lit(\"A\").alias(\"venue\"),\n",
    "      col(\"xg_away\").alias(\"xg_for\"),\n",
    "      col(\"xg_home\").alias(\"xg_against\"),\n",
    "      col(\"p_away_win\").alias(\"p_win\"),\n",
    "      col(\"p_draw\").alias(\"p_draw\"),\n",
    "      col(\"p_home_win\").alias(\"p_loss\"),\n",
    "      col(\"exp_pts_away\").alias(\"exp_pts\")\n",
    "    )\n",
    "\n",
    "  val teamFixtures =\n",
    "    homeView\n",
    "      .unionByName(awayView)\n",
    "      .withColumn(\"p_win\", round(col(\"p_win\") * 100, 2))\n",
    "      .withColumn(\"p_draw\", round(col(\"p_draw\") * 100, 2))\n",
    "      .withColumn(\"p_loss\", round(col(\"p_loss\") * 100, 2))\n",
    "      .withColumn(\"xg_for\", round(col(\"xg_for\"), 2))\n",
    "      .withColumn(\"xg_against\", round(col(\"xg_against\"), 2))\n",
    "      .withColumn(\"exp_pts\", round(col(\"exp_pts\"), 2))\n",
    "\n",
    "  // Debug view\n",
    "  teamFixtures.createOrReplaceTempView(\"fixtures_live\")\n",
    "\n",
    "  teamFixtures\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(FIXT_OUT)\n",
    "\n",
    "  dashboardBootstrapped = true\n",
    "  liveTable.unpersist()\n",
    "}\n",
    "\n",
    "// -----------------------------\n",
    "// Offline bootstrap: always produce outputs at least once from df/master.\n",
    "// -----------------------------\n",
    "// This ensures you get dashboard + fixtures CSVs even if:\n",
    "// - no streaming files arrive yet, or\n",
    "// - only the event stream is running.\n",
    "val AUTO_BOOTSTRAP_DASHBOARD = true\n",
    "\n",
    "if (AUTO_BOOTSTRAP_DASHBOARD && !dashboardBootstrapped) {\n",
    "  println(\"[AUTO] bootstrapping dashboard from df/master -> refreshDashboard()\")\n",
    "  ensureMasterParquet()\n",
    "  refreshDashboard(seasonLabel = \"25/26\", mcSims = DEFAULT_MC_SIMS)\n",
    "\n",
    "  // Print the key views so the initial (no-stream) run is visibly doing work.\n",
    "  if (spark.catalog.tableExists(\"dashboard_live\")) {\n",
    "    println(\"[AUTO] dashboard_live (top 10)\")\n",
    "    spark.table(\"dashboard_live\").show(10, truncate = false)\n",
    "  }\n",
    "  if (spark.catalog.tableExists(\"spi_live\")) {\n",
    "    println(\"[AUTO] spi_live (top 10)\")\n",
    "    spark.table(\"spi_live\").orderBy(desc(\"spi\")).show(10, truncate = false)\n",
    "  }\n",
    "  if (spark.catalog.tableExists(\"fixtures_live\")) {\n",
    "    println(\"[AUTO] fixtures_live (top 10)\")\n",
    "    spark.table(\"fixtures_live\").orderBy(\"match_date\").show(10, truncate = false)\n",
    "  }\n",
    "\n",
    "  println(\"[AUTO] bootstrap done\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae7ea388-7264-4258-b05a-70362a618d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "statsAccum = 0\n",
       "mlQ = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7cba14f6\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@7cba14f6"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[STATS] batchId=0 | inputRows=7\n",
      "[STATS] files:\n",
      "+-------------------------------------------------------------------------------------+\n",
      "|source_file                                                                          |\n",
      "+-------------------------------------------------------------------------------------+\n",
      "|file:///workspace/PowerRanking/data/stream_in/stats/stats_14195512_1766202822386.json|\n",
      "|file:///workspace/PowerRanking/data/stream_in/stats/stats_14195507_1766202822381.json|\n",
      "|file:///workspace/PowerRanking/data/stream_in/stats/stats_14195509_1766202822383.json|\n",
      "|file:///workspace/PowerRanking/data/stream_in/stats/stats_14195508_1766202822382.json|\n",
      "|file:///workspace/PowerRanking/data/stream_in/stats/stats_14195510_1766202822384.json|\n",
      "|file:///workspace/PowerRanking/data/stream_in/stats/stats_14195506_1766202822380.json|\n",
      "|file:///workspace/PowerRanking/data/stream_in/stats/stats_14195511_1766202822385.json|\n",
      "+-------------------------------------------------------------------------------------+\n",
      "\n",
      "=== stats batchId=0 | newUnique=7 ===\n",
      "[DATA] stats incoming unique=7 | matched_in_master=6 | unmatched=1 | labeled_rows_before=253\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.streaming.Trigger\n",
    "\n",
    "@volatile var statsAccum: Long = 0L\n",
    "\n",
    "val mlQ = statsStream.writeStream\n",
    "  .outputMode(\"append\")\n",
    "  // Poll for new files every 2s so \"drag in JSON\" demos reliably show new batches.\n",
    "  .trigger(Trigger.ProcessingTime(\"2 seconds\"))\n",
    "  .foreachBatch { (statsBatch: DataFrame, batchId: Long) =>\n",
    "\n",
    "    val inputRows = statsBatch.count()\n",
    "    println(s\"[STATS] batchId=$batchId | inputRows=$inputRows\")\n",
    "\n",
    "    if (inputRows == 0) {\n",
    "      // keep-alive tick\n",
    "    } else {\n",
    "      if (statsBatch.columns.contains(\"source_file\")) {\n",
    "        println(\"[STATS] files:\")\n",
    "        statsBatch.select(\"source_file\").distinct().show(50, truncate = false)\n",
    "      }\n",
    "\n",
    "      val newUnique = statsBatch.select(\"event_id\").distinct().count()\n",
    "      println(s\"=== stats batchId=$batchId | newUnique=$newUnique ===\")\n",
    "\n",
    "      // 1) upsert stats into the master parquet\n",
    "      upsertStatsIntoMaster(statsBatch)\n",
    "      println(s\"[DATA] upserted stats into $MASTER_PATH\")\n",
    "\n",
    "      // 2) update cumulative counter for periodic refit\n",
    "      statsAccum += newUnique\n",
    "      println(s\"[TRIGGER] accum=$statsAccum (refit when >= 4)\")\n",
    "\n",
    "      val LIVE_MC_SIMS = 2000\n",
    "      val threshold = 4L\n",
    "\n",
    "      // 3) refit periodically\n",
    "      if (statsAccum >= threshold) {\n",
    "        statsAccum -= threshold\n",
    "        println(s\"[ML] TRIGGER refit | carry remainder=$statsAccum\")\n",
    "\n",
    "        ensureMasterParquet()\n",
    "        val masterNow = spark.read.parquet(MASTER_PATH)\n",
    "\n",
    "        val modelDFnew = masterNow\n",
    "          .filter(col(\"home_expected_goals\").isNotNull && col(\"away_expected_goals\").isNotNull)\n",
    "          .dropDuplicates(\"event_id\")\n",
    "\n",
    "        val trainRows = modelDFnew.count()\n",
    "        println(s\"[ML] training rows = $trainRows\")\n",
    "\n",
    "        if (trainRows == 0) {\n",
    "          println(\"[ML] WARNING: 0 training rows (no rows in master with both xG labels).\")\n",
    "          println(\"[ML] This usually means: (1) event_id mismatch (check matched_in_master), or (2) master parquet was overwritten with null xG.\")\n",
    "          println(\"[ML] Skipping refit and keeping existing models.\")\n",
    "        } else {\n",
    "          val (model_home, model_away) = refitHomeAwayPipelines(modelDFnew)\n",
    "          model_home.write.overwrite().save(MODEL_HOME_PATH)\n",
    "          model_away.write.overwrite().save(MODEL_AWAY_PATH)\n",
    "          println(\"[ML] refit complete + models saved\")\n",
    "        }\n",
    "      } else {\n",
    "        println(\"[ML] skip refit\")\n",
    "      }\n",
    "\n",
    "      // 4) Always re-score + export so the dashboard reacts to new stats\n",
    "      refreshDashboard(seasonLabel = \"25/26\", mcSims = LIVE_MC_SIMS)\n",
    "\n",
    "      // 5) Print key views for demo\n",
    "      if (spark.catalog.tableExists(\"dashboard_live\")) {\n",
    "        println(\"[DEMO] dashboard_live (top 10)\")\n",
    "        spark.table(\"dashboard_live\").show(10, truncate = false)\n",
    "      }\n",
    "      if (spark.catalog.tableExists(\"spi_live\")) {\n",
    "        println(\"[DEMO] spi_live (top 10)\")\n",
    "        spark.table(\"spi_live\").orderBy(desc(\"spi\")).show(10, truncate = false)\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  // NO CHECKPOINTING (demo mode)\n",
    "  .start()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
