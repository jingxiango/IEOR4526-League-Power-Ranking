{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e22d77dc-fb38-4420-ba7d-429f04b87ad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lastException = null\n",
       "spark = org.apache.spark.sql.SparkSession@26770d9b\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@26770d9b"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.sql.{SparkSession, DataFrame}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.feature.{StringIndexer, OneHotEncoder, VectorAssembler}\n",
    "import org.apache.spark.ml.regression.GeneralizedLinearRegression\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "\n",
    "//create spark session\n",
    "val spark = SparkSession.builder.appName(\"SPL Analysis\").getOrCreate()\n",
    "import spark.implicits._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c289f2e0-0f7b-4903-bd86-ad66a2b67550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [season_id: int, season_label: string ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[season_id: int, season_label: string ... 8 more fields]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//import transformed data\n",
    "val df = spark.read\n",
    "  .format(\"csv\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .load(\"data/data_for_model.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae116edd-23d1-4ecc-b36f-4ff74d7e39dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "//to partquet for editing\n",
    "df.write\n",
    "  .mode(\"overwrite\")\n",
    "  .parquet(\"data/model_master\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8f98e170-a4c1-4c6b-b0d0-1116e517a47b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "currentTable = [team: string, pts: int ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---+---+---+\n",
      "|team              |pts|gf |ga |\n",
      "+------------------+---+---+---+\n",
      "|Lion City Sailors |15 |22 |2  |\n",
      "|Geylang Int.      |10 |7  |4  |\n",
      "|Tampines Rovers   |8  |8  |4  |\n",
      "|Balestier Khalsa  |8  |9  |12 |\n",
      "|Albirex Niigata   |7  |8  |5  |\n",
      "|Hougang Utd       |3  |4  |10 |\n",
      "|Tanjong Pagar Utd.|3  |7  |15 |\n",
      "|Young Lions       |0  |4  |17 |\n",
      "+------------------+---+---+---+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[team: string, pts: int ... 2 more fields]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//current league table\n",
    "//include goals for, goals against\n",
    "val currentTable = Seq(\n",
    "  (\"Lion City Sailors\", 15, 22, 2),\n",
    "  (\"Geylang Int.\", 10, 7, 4),\n",
    "  (\"Tampines Rovers\", 8, 8, 4),\n",
    "  (\"Balestier Khalsa\", 8, 9, 12),\n",
    "  (\"Albirex Niigata\", 7, 8, 5),\n",
    "  (\"Hougang Utd\", 3, 4, 10),\n",
    "  (\"Tanjong Pagar Utd.\", 3, 7, 15),\n",
    "  (\"Young Lions\", 0, 4, 17)\n",
    ").toDF(\"team\", \"pts\", \"gf\", \"ga\")\n",
    "currentTable.show(truncate = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d9d3c45f-9f6b-48e9-b516-b0eb43a2084e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eventSchema = StructType(StructField(event_id,LongType,true),StructField(season_year,StringType,true),StructField(home_team,StringType,true),StructField(away_team,StringType,true),StructField(home_score,LongType,true),StructField(away_score,LongType,true),StructField(status_code,LongType,true),StructField(start_timestamp,LongType,true))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StructType(StructField(event_id,LongType,true),StructField(season_year,StringType,true),StructField(home_team,StringType,true),StructField(away_team,StringType,true),StructField(home_score,LongType,true),StructField(away_score,LongType,true),StructField(status_code,LongType,true),StructField(start_timestamp,LongType,true))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//stream to update current table\n",
    "// NOTE: Avoid case class encoders in notebooks/REPL (can cause \"Unable to generate an encoder for inner class...\").\n",
    "// Use an explicit StructType schema instead.\n",
    "import org.apache.spark.sql.types._\n",
    "\n",
    "val eventSchema = StructType(Seq(\n",
    "  StructField(\"event_id\", LongType, nullable = true),\n",
    "  StructField(\"season_year\", StringType, nullable = true),\n",
    "  StructField(\"home_team\", StringType, nullable = true),\n",
    "  StructField(\"away_team\", StringType, nullable = true),\n",
    "  StructField(\"home_score\", LongType, nullable = true),\n",
    "  StructField(\"away_score\", LongType, nullable = true),\n",
    "  StructField(\"status_code\", LongType, nullable = true),\n",
    "  StructField(\"start_timestamp\", LongType, nullable = true)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9e76310-b858-4d56-8ce5-2be42f3d7c39",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "eventStream = [event_id: bigint, season_year: string ... 6 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[event_id: bigint, season_year: string ... 6 more fields]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val eventStream = spark.readStream\n",
    "  .schema(eventSchema)\n",
    "  .option(\"multiLine\", \"true\")\n",
    "  .json(\"data/stream_in/event\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "638ca077-15b4-4ded-8752-e55b52cab679",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "perMatchTeamDelta = [event_id: bigint, team: string ... 3 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[event_id: bigint, team: string ... 3 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//one match will affect 2 teams on the table\n",
    "val perMatchTeamDelta =\n",
    "  eventStream\n",
    "//get pts for home team\n",
    "    .withColumn(\n",
    "      \"home_pts\",\n",
    "      when(col(\"home_score\") > col(\"away_score\"), lit(3))\n",
    "        .when(col(\"home_score\") === col(\"away_score\"), lit(1))\n",
    "        .otherwise(lit(0))\n",
    "    )\n",
    "//get pts for away team\n",
    "    .withColumn(\n",
    "      \"away_pts\",\n",
    "      when(col(\"away_score\") > col(\"home_score\"), lit(3))\n",
    "        .when(col(\"away_score\") === col(\"home_score\"), lit(1))\n",
    "        .otherwise(lit(0))\n",
    "    )\n",
    "    .select(\n",
    "      col(\"event_id\"),\n",
    "      array(\n",
    "        struct(\n",
    "          col(\"home_team\").alias(\"team\"),\n",
    "          col(\"home_pts\").alias(\"pts_delta\"),\n",
    "          col(\"home_score\").alias(\"gf_delta\"),\n",
    "          col(\"away_score\").alias(\"ga_delta\")\n",
    "        ),\n",
    "        struct(\n",
    "          col(\"away_team\").alias(\"team\"),\n",
    "          col(\"away_pts\").alias(\"pts_delta\"),\n",
    "          col(\"away_score\").alias(\"gf_delta\"),\n",
    "          col(\"home_score\").alias(\"ga_delta\")\n",
    "        )\n",
    "      ).alias(\"rows\")\n",
    "    )\n",
    "    .withColumn(\"row\", explode(col(\"rows\")))\n",
    "    .select(\n",
    "      col(\"event_id\"),\n",
    "      col(\"row.team\").alias(\"team\"),\n",
    "      col(\"row.pts_delta\").alias(\"pts_delta\"),\n",
    "      col(\"row.gf_delta\").alias(\"gf_delta\"),\n",
    "      col(\"row.ga_delta\").alias(\"ga_delta\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17a48535-ee96-445f-8a7e-decc5fdca854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "aggregateDelta: (perMatchTeamDeltaBatch: org.apache.spark.sql.DataFrame)org.apache.spark.sql.DataFrame\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "// Aggregate changes (batch-level).\n",
    "// NOTE: We do this inside foreachBatch so we don't need streaming state/checkpoints.\n",
    "def aggregateDelta(perMatchTeamDeltaBatch: DataFrame): DataFrame = {\n",
    "  perMatchTeamDeltaBatch\n",
    "    .groupBy(col(\"team\"))\n",
    "    .agg(\n",
    "      sum(col(\"pts_delta\")).alias(\"pts_add\"),\n",
    "      sum(col(\"gf_delta\")).alias(\"gf_add\"),\n",
    "      sum(col(\"ga_delta\")).alias(\"ga_add\")\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abbcbe53-38e5-40df-811f-d5ebcda4ae5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "q = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@2cc335aa\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@2cc335aa"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== batchId=0 ===\n",
      "+------------------+---+---+---+\n",
      "|team              |pts|gf |ga |\n",
      "+------------------+---+---+---+\n",
      "|Lion City Sailors |27 |30 |2  |\n",
      "|Tampines Rovers   |16 |20 |16 |\n",
      "|Hougang Utd       |15 |8  |10 |\n",
      "|Tanjong Pagar Utd.|15 |19 |23 |\n",
      "|Geylang Int.      |14 |15 |20 |\n",
      "|Albirex Niigata   |11 |12 |9  |\n",
      "|Balestier Khalsa  |8  |9  |16 |\n",
      "|Young Lions       |0  |12 |29 |\n",
      "+------------------+---+---+---+\n",
      "\n",
      "=== batchId=1 ===\n",
      "+------------------+---+---+---+\n",
      "|team              |pts|gf |ga |\n",
      "+------------------+---+---+---+\n",
      "|Lion City Sailors |30 |32 |2  |\n",
      "|Tampines Rovers   |16 |20 |16 |\n",
      "|Hougang Utd       |15 |8  |10 |\n",
      "|Tanjong Pagar Utd.|15 |19 |23 |\n",
      "|Geylang Int.      |14 |15 |22 |\n",
      "|Albirex Niigata   |11 |12 |9  |\n",
      "|Balestier Khalsa  |8  |9  |16 |\n",
      "|Young Lions       |0  |12 |29 |\n",
      "+------------------+---+---+---+\n",
      "\n",
      "=== batchId=2 ===\n",
      "+------------------+---+---+---+\n",
      "|team              |pts|gf |ga |\n",
      "+------------------+---+---+---+\n",
      "|Lion City Sailors |30 |32 |2  |\n",
      "|Tampines Rovers   |18 |23 |19 |\n",
      "|Hougang Utd       |18 |9  |10 |\n",
      "|Tanjong Pagar Utd.|18 |22 |25 |\n",
      "|Geylang Int.      |15 |17 |24 |\n",
      "|Albirex Niigata   |12 |13 |10 |\n",
      "|Balestier Khalsa  |8  |9  |17 |\n",
      "|Young Lions       |0  |14 |32 |\n",
      "+------------------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "\n",
    "// NO CHECKPOINTING (demo mode): do all aggregation/update per micro-batch.\n",
    "val q = perMatchTeamDelta.writeStream\n",
    "  .outputMode(\"append\")\n",
    "  .foreachBatch { (batchRows: DataFrame, batchId: Long) =>\n",
    "\n",
    "    val batchDelta = aggregateDelta(batchRows)\n",
    "\n",
    "    val baseTable =\n",
    "      if (spark.catalog.tableExists(\"current_table_live\")) spark.table(\"current_table_live\")\n",
    "      else currentTable\n",
    "\n",
    "    val updated =\n",
    "      baseTable.as(\"c\")\n",
    "        .join(batchDelta.as(\"d\"), Seq(\"team\"), \"left\")\n",
    "        .select(\n",
    "          col(\"team\"),\n",
    "          (col(\"c.pts\") + coalesce(col(\"d.pts_add\"), lit(0))).alias(\"pts\"),\n",
    "          (col(\"c.gf\")  + coalesce(col(\"d.gf_add\"),  lit(0))).alias(\"gf\"),\n",
    "          (col(\"c.ga\")  + coalesce(col(\"d.ga_add\"),  lit(0))).alias(\"ga\")\n",
    "        )\n",
    "        .orderBy(desc(\"pts\"), (col(\"gf\") - col(\"ga\")).desc, desc(\"gf\"))\n",
    "\n",
    "    updated.createOrReplaceTempView(\"current_table_live\")\n",
    "    println(s\"=== batchId=$batchId ===\")\n",
    "    updated.show(50, truncate = false)\n",
    "  }\n",
    "  .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c2086d3-8197-4fdb-8067-528396621e02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "statsSchema = StructType(StructField(event_id,LongType,true),StructField(home_xg,DoubleType,true),StructField(away_xg,DoubleType,true))\n",
       "statsStream = [event_id: bigint, home_xg: double ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[event_id: bigint, home_xg: double ... 1 more field]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//stats streaming\n",
    "// NOTE: Avoid case class encoders in notebooks/REPL; use explicit schema.\n",
    "val statsSchema = StructType(Seq(\n",
    "  StructField(\"event_id\", LongType, nullable = true),\n",
    "  StructField(\"home_xg\", DoubleType, nullable = true),\n",
    "  StructField(\"away_xg\", DoubleType, nullable = true)\n",
    "))\n",
    "\n",
    "val statsStream = spark.readStream\n",
    "  .schema(statsSchema)\n",
    "  .option(\"multiLine\", \"true\")\n",
    "  .json(\"data/stream_in/stats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4029557b-9d27-4209-8272-6251e20fea54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "upsertStatsIntoMaster: (statsBatch: org.apache.spark.sql.DataFrame)Unit\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import org.apache.spark.sql.DataFrame\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "def upsertStatsIntoMaster(statsBatch: DataFrame): Unit = {\n",
    "\n",
    "  val masterPath = \"data/model_master\"\n",
    "\n",
    "  // Initialize master parquet if it doesn't exist yet (seed from the base CSV df).\n",
    "  // In this project we use a parquet folder at `data/model_master` as the streaming-updated master.\n",
    "  val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)\n",
    "  val p = new Path(masterPath)\n",
    "\n",
    "  val exists = fs.exists(p)\n",
    "  val empty = exists && {\n",
    "    val st = fs.listStatus(p)\n",
    "    st == null || st.isEmpty\n",
    "  }\n",
    "\n",
    "  if (!exists || empty) {\n",
    "    if (empty) {\n",
    "      fs.delete(p, true)\n",
    "      println(s\"[INIT] $masterPath existed but was empty -> recreating from df\")\n",
    "    } else {\n",
    "      println(s\"[INIT] $masterPath not found -> creating from df (data_for_model.csv)\")\n",
    "    }\n",
    "    df.write.mode(\"overwrite\").parquet(masterPath)\n",
    "  }\n",
    "\n",
    "  // Rename incoming columns to avoid duplicate column names after join.\n",
    "  // We only want to fill nulls in the master (not overwrite existing xG).\n",
    "  val updates = statsBatch\n",
    "    .select(\n",
    "      col(\"event_id\"),\n",
    "      col(\"home_xg\").cast(\"double\").alias(\"u_home_expected_goals\"),\n",
    "      col(\"away_xg\").cast(\"double\").alias(\"u_away_expected_goals\")\n",
    "    )\n",
    "    .dropDuplicates(\"event_id\")\n",
    "\n",
    "  val master = spark.read.parquet(masterPath)\n",
    "\n",
    "  val updated =\n",
    "    master.as(\"m\")\n",
    "      .join(updates.as(\"u\"), Seq(\"event_id\"), \"left\")\n",
    "      .withColumn(\n",
    "        \"home_expected_goals\",\n",
    "        coalesce(col(\"m.home_expected_goals\"), col(\"u.u_home_expected_goals\"))\n",
    "      )\n",
    "      .withColumn(\n",
    "        \"away_expected_goals\",\n",
    "        coalesce(col(\"m.away_expected_goals\"), col(\"u.u_away_expected_goals\"))\n",
    "      )\n",
    "      .withColumn(\n",
    "        \"is_future_fixture\",\n",
    "        when(col(\"u.event_id\").isNotNull, lit(0)) // stats arrived → no longer future\n",
    "          .otherwise(col(\"m.is_future_fixture\"))\n",
    "      )\n",
    "      .drop(\"u_home_expected_goals\", \"u_away_expected_goals\")\n",
    "\n",
    "  updated.write.mode(\"overwrite\").parquet(masterPath)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8039a9c-d1d5-4392-ab4c-26bd1f4840ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AUTO] bootstrapping dashboard from df/master -> refreshDashboard()\n",
      "[AUTO] bootstrap done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MASTER_PATH = data/model_master\n",
       "MODEL_HOME_PATH = data/models/glm_home_pipeline_latest\n",
       "MODEL_AWAY_PATH = data/models/glm_away_pipeline_latest\n",
       "DASH_OUT = data/dashboard_table_csv\n",
       "FIXT_OUT = data/match_level_fixtures\n",
       "DEFAULT_MC_SIMS = 20000\n",
       "dashboardBootstrapped = true\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: four deprecations (since 3.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n",
       "refitHomeAwayPipelines: (trainRaw: org.apache.spark.sql.DataFrame)(org.apache.spark.ml.PipelineModel, org.apache.spark.ml.PipelineModel)\n",
       "ensureMasterParquet: ()Unit\n",
       "tryLoadPipeline: (path: String)Option[org.apache.spark.ml.PipelineModel]\n",
       "loadOrTrainPipelines: ()(org.apache.spark.m...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "//pipeline refit function for home + away GLM\n",
    "def refitHomeAwayPipelines(trainRaw: DataFrame): (PipelineModel, PipelineModel) = {\n",
    "  // IMPORTANT: GLM label columns are home_xg / away_xg\n",
    "  // but the master parquet stores them as home_expected_goals / away_expected_goals.\n",
    "  val train = trainRaw.select(\n",
    "    col(\"home_team\"),\n",
    "    col(\"away_team\"),\n",
    "    col(\"home_expected_goals\").cast(\"double\").alias(\"home_xg\"),\n",
    "    col(\"away_expected_goals\").cast(\"double\").alias(\"away_xg\"),\n",
    "    lit(1.0).alias(\"home_adv\")\n",
    "  )\n",
    "\n",
    "  val home_idx = new StringIndexer()\n",
    "    .setInputCol(\"home_team\")\n",
    "    .setOutputCol(\"home_team_idx\")\n",
    "    .setHandleInvalid(\"keep\")\n",
    "\n",
    "  val away_idx = new StringIndexer()\n",
    "    .setInputCol(\"away_team\")\n",
    "    .setOutputCol(\"away_team_idx\")\n",
    "    .setHandleInvalid(\"keep\")\n",
    "\n",
    "  val enc = new OneHotEncoder()\n",
    "    .setInputCols(Array(\"home_team_idx\", \"away_team_idx\"))\n",
    "    .setOutputCols(Array(\"home_team_ohe\", \"away_team_ohe\"))\n",
    "\n",
    "  val assembler = new VectorAssembler()\n",
    "    .setInputCols(Array(\"home_team_ohe\", \"away_team_ohe\", \"home_adv\"))\n",
    "    .setOutputCol(\"features\")\n",
    "\n",
    "  val glm_home = new GeneralizedLinearRegression()\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setLabelCol(\"home_xg\")\n",
    "    .setFamily(\"gaussian\")\n",
    "    .setLink(\"identity\")\n",
    "    .setMaxIter(50)\n",
    "    .setRegParam(0.0)\n",
    "\n",
    "  val glm_away = new GeneralizedLinearRegression()\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setLabelCol(\"away_xg\")\n",
    "    .setFamily(\"gaussian\")\n",
    "    .setLink(\"identity\")\n",
    "    .setMaxIter(50)\n",
    "    .setRegParam(0.0)\n",
    "\n",
    "  val pipe_home = new Pipeline().setStages(Array(home_idx, away_idx, enc, assembler, glm_home))\n",
    "  val pipe_away = new Pipeline().setStages(Array(home_idx, away_idx, enc, assembler, glm_away))\n",
    "\n",
    "  val model_home = pipe_home.fit(train)\n",
    "  val model_away = pipe_away.fit(train)\n",
    "\n",
    "  (model_home, model_away)\n",
    "}\n",
    "\n",
    "// -----------------------------\n",
    "// Pattern B helpers: load-or-train + refresh exports\n",
    "// -----------------------------\n",
    "// (PipelineModel is imported in Cell 0)\n",
    "import org.apache.spark.ml.feature.{StringIndexerModel, OneHotEncoderModel}\n",
    "import org.apache.spark.ml.regression.GeneralizedLinearRegressionModel\n",
    "\n",
    "val MASTER_PATH = \"data/model_master\"\n",
    "val MODEL_HOME_PATH = \"data/models/glm_home_pipeline_latest\"\n",
    "val MODEL_AWAY_PATH = \"data/models/glm_away_pipeline_latest\"\n",
    "val DASH_OUT = \"data/dashboard_table_csv\"\n",
    "val FIXT_OUT = \"data/match_level_fixtures\"\n",
    "\n",
    "// Keep these small-ish for \"live\" refresh. You can bump them if runtime is OK.\n",
    "val DEFAULT_MC_SIMS = 20000\n",
    "\n",
    "@volatile var dashboardBootstrapped: Boolean = false\n",
    "\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "def ensureMasterParquet(): Unit = {\n",
    "  val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)\n",
    "  val p = new Path(MASTER_PATH)\n",
    "  if (!fs.exists(p)) {\n",
    "    println(s\"[INIT] $MASTER_PATH not found -> creating from df (data_for_model.csv)\")\n",
    "    df.write.mode(\"overwrite\").parquet(MASTER_PATH)\n",
    "  }\n",
    "}\n",
    "\n",
    "def tryLoadPipeline(path: String): Option[PipelineModel] = {\n",
    "  try Some(PipelineModel.load(path))\n",
    "  catch { case _: Throwable => None }\n",
    "}\n",
    "\n",
    "def loadOrTrainPipelines(): (PipelineModel, PipelineModel) = {\n",
    "  val mh = tryLoadPipeline(MODEL_HOME_PATH)\n",
    "  val ma = tryLoadPipeline(MODEL_AWAY_PATH)\n",
    "  if (mh.isDefined && ma.isDefined) return (mh.get, ma.get)\n",
    "\n",
    "  ensureMasterParquet()\n",
    "  val master = spark.read.parquet(MASTER_PATH)\n",
    "  val modelDF = master\n",
    "    .filter(col(\"home_expected_goals\").isNotNull && col(\"away_expected_goals\").isNotNull)\n",
    "    .dropDuplicates(\"event_id\")\n",
    "\n",
    "  val (h, a) = refitHomeAwayPipelines(modelDF)\n",
    "  h.write.overwrite().save(MODEL_HOME_PATH)\n",
    "  a.write.overwrite().save(MODEL_AWAY_PATH)\n",
    "  (h, a)\n",
    "}\n",
    "\n",
    "def getLiveTableSafe(): DataFrame = {\n",
    "  if (spark.catalog.tableExists(\"current_table_live\")) spark.table(\"current_table_live\")\n",
    "  else currentTable\n",
    "}\n",
    "\n",
    "def powerRankFromHomePipeline(modelHome: PipelineModel): DataFrame = {\n",
    "  val stages = modelHome.stages\n",
    "  val homeIdx = stages(0).asInstanceOf[StringIndexerModel]\n",
    "  val awayIdx = stages(1).asInstanceOf[StringIndexerModel]\n",
    "  val ohe = stages(2).asInstanceOf[OneHotEncoderModel]\n",
    "  val glm = stages(4).asInstanceOf[GeneralizedLinearRegressionModel]\n",
    "\n",
    "  val coeff = glm.coefficients.toArray\n",
    "  val homeCats = ohe.categorySizes(0)\n",
    "  val awayCats = ohe.categorySizes(1)\n",
    "  val dropLast = ohe.getDropLast\n",
    "  val homeDim = if (dropLast) homeCats - 1 else homeCats\n",
    "  val awayDim = if (dropLast) awayCats - 1 else awayCats\n",
    "\n",
    "  val homeAttackCoef = coeff.slice(0, homeDim)\n",
    "  val awayDefenseCoef = coeff.slice(homeDim, homeDim + awayDim)\n",
    "\n",
    "  val homeTeams = if (homeIdx.getHandleInvalid == \"keep\") homeIdx.labels :+ \"UNKNOWN\" else homeIdx.labels\n",
    "  val awayTeams = if (awayIdx.getHandleInvalid == \"keep\") awayIdx.labels :+ \"UNKNOWN\" else awayIdx.labels\n",
    "\n",
    "  def attachBaseline(teams: Array[String], coef: Array[Double], dropLast: Boolean): Seq[(String, Double)] =\n",
    "    if (dropLast) teams.zip(coef :+ 0.0) else teams.zip(coef)\n",
    "\n",
    "  val homeAttack = attachBaseline(homeTeams, homeAttackCoef, dropLast).toDF(\"team\", \"home_attack\")\n",
    "  val awayDefWeak = attachBaseline(awayTeams, awayDefenseCoef, dropLast).toDF(\"team\", \"away_def_weak\")\n",
    "\n",
    "  homeAttack\n",
    "    .join(awayDefWeak, \"team\")\n",
    "    .withColumn(\"power_score\", col(\"home_attack\") - col(\"away_def_weak\"))\n",
    "    .orderBy(desc(\"power_score\"))\n",
    "}\n",
    "\n",
    "// Full refresh: score fixtures -> compute odds/MC/SPI -> export CSVs for Streamlit\n",
    "// This is what makes Pattern B non-redundant: predictions always come from the latest saved pipeline models.\n",
    "def refreshDashboard(seasonLabel: String = \"25/26\", mcSims: Int = DEFAULT_MC_SIMS): Unit = {\n",
    "  val liveTable = getLiveTableSafe().cache()\n",
    "\n",
    "  val (modelHome, modelAway) = loadOrTrainPipelines()\n",
    "  ensureMasterParquet()\n",
    "  val master = spark.read.parquet(MASTER_PATH)\n",
    "\n",
    "  // 1) Score future fixtures\n",
    "  val fixtures = master\n",
    "    .filter(col(\"is_future_fixture\") === 1 && col(\"season_label\") === seasonLabel)\n",
    "    .select(\n",
    "      col(\"event_id\"),\n",
    "      col(\"match_date\"),\n",
    "      trim(col(\"home_team\")).alias(\"home_team\"),\n",
    "      trim(col(\"away_team\")).alias(\"away_team\"),\n",
    "      lit(1.0).alias(\"home_adv\")\n",
    "    )\n",
    "    .dropDuplicates(\"event_id\")\n",
    "\n",
    "  // IMPORTANT: modelHome and modelAway are both Pipelines that create the same intermediate columns\n",
    "  // (home_team_idx, away_team_idx, *ohe, features). If we feed the full output of modelHome into modelAway,\n",
    "  // the second transform fails with: \"Output column home_team_idx already exists\".\n",
    "  val predHomeFull = modelHome.transform(fixtures)\n",
    "    .withColumnRenamed(\"prediction\", \"lambda_home\")\n",
    "\n",
    "  // Drop intermediate pipeline columns by selecting only what modelAway needs.\n",
    "  val predHome = predHomeFull.select(\n",
    "    col(\"event_id\"),\n",
    "    col(\"match_date\"),\n",
    "    col(\"home_team\"),\n",
    "    col(\"away_team\"),\n",
    "    col(\"home_adv\"),\n",
    "    col(\"lambda_home\")\n",
    "  )\n",
    "\n",
    "  val predBothFull = modelAway.transform(predHome)\n",
    "    .withColumnRenamed(\"prediction\", \"lambda_away\")\n",
    "\n",
    "  val predBoth = predBothFull.select(\n",
    "    col(\"event_id\"),\n",
    "    col(\"match_date\"),\n",
    "    col(\"home_team\"),\n",
    "    col(\"away_team\"),\n",
    "    col(\"lambda_home\"),\n",
    "    col(\"lambda_away\")\n",
    "  )\n",
    "\n",
    "  // 2) W/D/L probabilities (local UDF to avoid global symbol clashes)\n",
    "  import scala.math.{exp, pow}\n",
    "\n",
    "  def poissonPmf(k: Int, lambda: Double): Double = {\n",
    "    if (k < 0) 0.0\n",
    "    else {\n",
    "      var fact = 1.0\n",
    "      var i = 2\n",
    "      while (i <= k) { fact *= i; i += 1 }\n",
    "      exp(-lambda) * pow(lambda, k) / fact\n",
    "    }\n",
    "  }\n",
    "\n",
    "  case class WDL(win: Double, draw: Double, loss: Double)\n",
    "\n",
    "  def wdlFromLambdas(lhRaw: Double, laRaw: Double, maxGoals: Int = 10): WDL = {\n",
    "    val lh = math.max(lhRaw, 0.0)\n",
    "    val la = math.max(laRaw, 0.0)\n",
    "\n",
    "    val ph = Array.tabulate(maxGoals + 1)(k => poissonPmf(k, lh))\n",
    "    val pa = Array.tabulate(maxGoals + 1)(k => poissonPmf(k, la))\n",
    "\n",
    "    var mass = 0.0\n",
    "    var i = 0\n",
    "    while (i <= maxGoals) {\n",
    "      var j = 0\n",
    "      while (j <= maxGoals) { mass += ph(i) * pa(j); j += 1 }\n",
    "      i += 1\n",
    "    }\n",
    "    val denom = if (mass > 0) mass else 1.0\n",
    "\n",
    "    var win = 0.0\n",
    "    var draw = 0.0\n",
    "    i = 0\n",
    "    while (i <= maxGoals) {\n",
    "      var j = 0\n",
    "      while (j <= maxGoals) {\n",
    "        val p = (ph(i) * pa(j)) / denom\n",
    "        if (i > j) win += p\n",
    "        else if (i == j) draw += p\n",
    "        j += 1\n",
    "      }\n",
    "      i += 1\n",
    "    }\n",
    "    WDL(win, draw, 1.0 - win - draw)\n",
    "  }\n",
    "\n",
    "  val wdlUdfLocal = udf((lh: Double, la: Double) => {\n",
    "    val r = wdlFromLambdas(lh, la, maxGoals = 10)\n",
    "    (r.win, r.draw, r.loss)\n",
    "  })\n",
    "\n",
    "  val matchProbs =\n",
    "    predBoth\n",
    "      .withColumn(\"wdl\", wdlUdfLocal(col(\"lambda_home\"), col(\"lambda_away\")))\n",
    "      .withColumn(\"p_home_win\", col(\"wdl\").getField(\"_1\"))\n",
    "      .withColumn(\"p_draw\",     col(\"wdl\").getField(\"_2\"))\n",
    "      .withColumn(\"p_away_win\", col(\"wdl\").getField(\"_3\"))\n",
    "      .drop(\"wdl\")\n",
    "      .withColumn(\"exp_pts_home\", lit(3.0) * col(\"p_home_win\") + lit(1.0) * col(\"p_draw\"))\n",
    "      .withColumn(\"exp_pts_away\", lit(3.0) * col(\"p_away_win\") + lit(1.0) * col(\"p_draw\"))\n",
    "      .withColumnRenamed(\"lambda_home\", \"xg_home\")\n",
    "      .withColumnRenamed(\"lambda_away\", \"xg_away\")\n",
    "\n",
    "  // 3) Monte Carlo season sims (using predicted lambdas)\n",
    "  import scala.util.Random\n",
    "\n",
    "  case class TeamState(var pts: Int, var gf: Int, var ga: Int)\n",
    "\n",
    "  def poisson(lambdaRaw: Double, rng: Random): Int = {\n",
    "    val lambda = math.max(lambdaRaw, 0.0)\n",
    "    val L = math.exp(-lambda)\n",
    "    var k = 0\n",
    "    var p = 1.0\n",
    "    do {\n",
    "      k += 1\n",
    "      p *= rng.nextDouble()\n",
    "    } while (p > L && k < 25)\n",
    "    k - 1\n",
    "  }\n",
    "\n",
    "  def goalDiff(s: TeamState): Int = s.gf - s.ga\n",
    "\n",
    "  def simulateSeasonOnce(\n",
    "    teams: Array[String],\n",
    "    base: Map[String, TeamState],\n",
    "    matches: Array[(String, String, Double, Double)],\n",
    "    seed: Int\n",
    "  ): Map[String, TeamState] = {\n",
    "\n",
    "    val rng = new Random(seed)\n",
    "\n",
    "    val st = scala.collection.mutable.Map[String, TeamState]()\n",
    "    teams.foreach { t =>\n",
    "      val b = base(t)\n",
    "      st(t) = TeamState(b.pts, b.gf, b.ga)\n",
    "    }\n",
    "\n",
    "    matches.foreach { case (h, a, lamH, lamA) =>\n",
    "      val gh = poisson(lamH, rng)\n",
    "      val ga = poisson(lamA, rng)\n",
    "\n",
    "      st(h).gf += gh; st(h).ga += ga\n",
    "      st(a).gf += ga; st(a).ga += gh\n",
    "\n",
    "      if (gh > ga) st(h).pts += 3\n",
    "      else if (gh < ga) st(a).pts += 3\n",
    "      else { st(h).pts += 1; st(a).pts += 1 }\n",
    "    }\n",
    "\n",
    "    st.toMap\n",
    "  }\n",
    "\n",
    "  val teams: Array[String] = liveTable.select($\"team\").as[String].collect()\n",
    "\n",
    "  val baseTable: Map[String, TeamState] =\n",
    "    liveTable.select($\"team\", $\"pts\", $\"gf\", $\"ga\").as[(String, Long, Long, Long)].collect()\n",
    "      .map { case (t, p, gf, ga) => t -> TeamState(p.toInt, gf.toInt, ga.toInt) }\n",
    "      .toMap\n",
    "\n",
    "  val matchesArr: Array[(String, String, Double, Double)] =\n",
    "    predBoth.select($\"home_team\", $\"away_team\", $\"lambda_home\", $\"lambda_away\")\n",
    "      .as[(String, String, Double, Double)]\n",
    "      .collect()\n",
    "\n",
    "  val winLeague = scala.collection.mutable.Map[String, Int]().withDefaultValue(0)\n",
    "  val makeACL   = scala.collection.mutable.Map[String, Int]().withDefaultValue(0)\n",
    "  val totalFinalPts = scala.collection.mutable.Map[String, Double]().withDefaultValue(0.0)\n",
    "\n",
    "  val N = math.max(mcSims, 1)\n",
    "\n",
    "  for (i <- 0 until N) {\n",
    "    val end = simulateSeasonOnce(teams, baseTable, matchesArr, seed = 1234 + i)\n",
    "\n",
    "    val ranked = teams.sortBy { t =>\n",
    "      val s = end(t)\n",
    "      (-s.pts, -goalDiff(s), -s.gf, t)\n",
    "    }\n",
    "\n",
    "    winLeague(ranked(0)) += 1\n",
    "    ranked.take(2).foreach(t => makeACL(t) += 1)\n",
    "    teams.foreach { t => totalFinalPts(t) += end(t).pts }\n",
    "  }\n",
    "\n",
    "  val mcPtsDF =\n",
    "    teams.toSeq.map { t =>\n",
    "      val expFinal = totalFinalPts(t) / N\n",
    "      (t, expFinal)\n",
    "    }.toDF(\"team\", \"exp_pts_final_mc\")\n",
    "\n",
    "  val mcRemainingDF =\n",
    "    mcPtsDF\n",
    "      .join(liveTable.select($\"team\", $\"pts\"), Seq(\"team\"))\n",
    "      .withColumn(\"exp_pts_remaining_mc\", col(\"exp_pts_final_mc\") - col(\"pts\"))\n",
    "\n",
    "  val probsDF =\n",
    "    teams.toSeq.map { t =>\n",
    "      val pWin = winLeague(t).toDouble / N\n",
    "      val pACL = makeACL(t).toDouble / N\n",
    "      (t, pWin * 100.0, pACL * 100.0)\n",
    "    }.toDF(\"team\", \"win_league_pct\", \"make_acl_pct\")\n",
    "\n",
    "  // 4) SPI from power rank\n",
    "  val powerRank = powerRankFromHomePipeline(modelHome)\n",
    "  val w = Window.partitionBy()\n",
    "  val spiDF =\n",
    "    powerRank\n",
    "      .withColumn(\"mu\", avg($\"power_score\").over(w))\n",
    "      .withColumn(\"sd\", stddev($\"power_score\").over(w))\n",
    "      .withColumn(\"spi\", lit(75.0) + lit(10.0) * (($\"power_score\" - $\"mu\") / $\"sd\"))\n",
    "      .select(\"team\", \"spi\")\n",
    "\n",
    "  // Debug views (so you can show() them in separate cells)\n",
    "  powerRank.createOrReplaceTempView(\"power_rank_live\")\n",
    "  spiDF.createOrReplaceTempView(\"spi_live\")\n",
    "\n",
    "  // 5) Dashboard table export\n",
    "  val core =\n",
    "    liveTable\n",
    "      .select(\n",
    "        col(\"team\"),\n",
    "        col(\"pts\").cast(\"double\").alias(\"pts\"),\n",
    "        col(\"gf\").cast(\"int\").alias(\"gf\"),\n",
    "        col(\"ga\").cast(\"int\").alias(\"ga\")\n",
    "      )\n",
    "\n",
    "  val finalTable =\n",
    "    core\n",
    "      .join(mcRemainingDF.select(\"team\", \"exp_pts_remaining_mc\"), Seq(\"team\"), \"left\")\n",
    "      .join(mcPtsDF.select(\"team\", \"exp_pts_final_mc\"), Seq(\"team\"), \"left\")\n",
    "      .join(probsDF.select(\"team\", \"win_league_pct\", \"make_acl_pct\"), Seq(\"team\"), \"left\")\n",
    "      .join(spiDF.select(\"team\", \"spi\"), Seq(\"team\"), \"left\")\n",
    "      .na.fill(0.0, Seq(\"exp_pts_remaining_mc\", \"exp_pts_final_mc\", \"win_league_pct\", \"make_acl_pct\", \"spi\"))\n",
    "      .orderBy(desc(\"exp_pts_final_mc\"))\n",
    "\n",
    "  // Debug view\n",
    "  finalTable.createOrReplaceTempView(\"dashboard_live\")\n",
    "\n",
    "  finalTable\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(DASH_OUT)\n",
    "\n",
    "  // 6) Match-level fixtures export (team/opponent rows)\n",
    "  val homeView =\n",
    "    matchProbs.select(\n",
    "      col(\"event_id\"),\n",
    "      col(\"match_date\"),\n",
    "      col(\"home_team\").alias(\"team\"),\n",
    "      col(\"away_team\").alias(\"opponent\"),\n",
    "      lit(\"H\").alias(\"venue\"),\n",
    "      col(\"xg_home\").alias(\"xg_for\"),\n",
    "      col(\"xg_away\").alias(\"xg_against\"),\n",
    "      col(\"p_home_win\").alias(\"p_win\"),\n",
    "      col(\"p_draw\").alias(\"p_draw\"),\n",
    "      col(\"p_away_win\").alias(\"p_loss\"),\n",
    "      col(\"exp_pts_home\").alias(\"exp_pts\")\n",
    "    )\n",
    "\n",
    "  val awayView =\n",
    "    matchProbs.select(\n",
    "      col(\"event_id\"),\n",
    "      col(\"match_date\"),\n",
    "      col(\"away_team\").alias(\"team\"),\n",
    "      col(\"home_team\").alias(\"opponent\"),\n",
    "      lit(\"A\").alias(\"venue\"),\n",
    "      col(\"xg_away\").alias(\"xg_for\"),\n",
    "      col(\"xg_home\").alias(\"xg_against\"),\n",
    "      col(\"p_away_win\").alias(\"p_win\"),\n",
    "      col(\"p_draw\").alias(\"p_draw\"),\n",
    "      col(\"p_home_win\").alias(\"p_loss\"),\n",
    "      col(\"exp_pts_away\").alias(\"exp_pts\")\n",
    "    )\n",
    "\n",
    "  val teamFixtures =\n",
    "    homeView\n",
    "      .unionByName(awayView)\n",
    "      .withColumn(\"p_win\", round(col(\"p_win\") * 100, 2))\n",
    "      .withColumn(\"p_draw\", round(col(\"p_draw\") * 100, 2))\n",
    "      .withColumn(\"p_loss\", round(col(\"p_loss\") * 100, 2))\n",
    "      .withColumn(\"xg_for\", round(col(\"xg_for\"), 2))\n",
    "      .withColumn(\"xg_against\", round(col(\"xg_against\"), 2))\n",
    "      .withColumn(\"exp_pts\", round(col(\"exp_pts\"), 2))\n",
    "\n",
    "  // Debug view\n",
    "  teamFixtures.createOrReplaceTempView(\"fixtures_live\")\n",
    "\n",
    "  teamFixtures\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .csv(FIXT_OUT)\n",
    "\n",
    "  dashboardBootstrapped = true\n",
    "  liveTable.unpersist()\n",
    "}\n",
    "\n",
    "// -----------------------------\n",
    "// Offline bootstrap: always produce outputs at least once from df/master.\n",
    "// -----------------------------\n",
    "// This ensures you get dashboard + fixtures CSVs even if:\n",
    "// - no streaming files arrive yet, or\n",
    "// - only the event stream is running.\n",
    "val AUTO_BOOTSTRAP_DASHBOARD = true\n",
    "\n",
    "if (AUTO_BOOTSTRAP_DASHBOARD && !dashboardBootstrapped) {\n",
    "  println(\"[AUTO] bootstrapping dashboard from df/master -> refreshDashboard()\")\n",
    "  ensureMasterParquet()\n",
    "  refreshDashboard(seasonLabel = \"25/26\", mcSims = DEFAULT_MC_SIMS)\n",
    "  println(\"[AUTO] bootstrap done\")\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7ea388-7264-4258-b05a-70362a618d59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "statsAccum = 0\n",
       "mlQ = org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@12654214\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.execution.streaming.StreamingQueryWrapper@12654214"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== stats batchId=0 | newUnique=5 ===\n"
     ]
    }
   ],
   "source": [
    "@volatile var statsAccum: Long = 0L\n",
    "\n",
    "val mlQ = statsStream.writeStream\n",
    "  .outputMode(\"append\")\n",
    "  .foreachBatch { (statsBatch: DataFrame, batchId: Long) =>\n",
    "\n",
    "    val newUnique = statsBatch.select(\"event_id\").distinct().count()\n",
    "    println(s\"=== stats batchId=$batchId | newUnique=$newUnique ===\")\n",
    "\n",
    "    if (newUnique == 0) {\n",
    "      // no-op\n",
    "    } else {\n",
    "      // 1) upsert stats into the master parquet\n",
    "      upsertStatsIntoMaster(statsBatch)\n",
    "      println(s\"[DATA] upserted stats into $MASTER_PATH\")\n",
    "\n",
    "      // 2) Always (re)export after new stats arrive so the professor sees changes immediately.\n",
    "      // Keep MC sims lower for interactive streaming updates.\n",
    "      val LIVE_MC_SIMS = 2000\n",
    "\n",
    "      if (!dashboardBootstrapped) {\n",
    "        println(\"[BOOT] no dashboard yet -> export dashboard/fixtures\")\n",
    "        refreshDashboard(seasonLabel = \"25/26\", mcSims = LIVE_MC_SIMS)\n",
    "      } else {\n",
    "        // 3) update cumulative counter for periodic refit\n",
    "        statsAccum += newUnique\n",
    "        println(s\"[TRIGGER] accum=$statsAccum (refit when >= 4)\")\n",
    "\n",
    "        if (statsAccum >= 4) {\n",
    "          // consume 4 and keep remainder\n",
    "          statsAccum -= 4\n",
    "          println(s\"[ML] TRIGGER refit | carry remainder=$statsAccum\")\n",
    "\n",
    "          // 4) refit on the UPDATED master DF (full history with xG)\n",
    "          ensureMasterParquet()\n",
    "          val masterNow = spark.read.parquet(MASTER_PATH)\n",
    "\n",
    "          val modelDF = masterNow\n",
    "            .filter(col(\"home_expected_goals\").isNotNull && col(\"away_expected_goals\").isNotNull)\n",
    "            .dropDuplicates(\"event_id\")\n",
    "\n",
    "          println(s\"[ML] training rows = ${modelDF.count()}\")\n",
    "\n",
    "          val (model_home, model_away) = refitHomeAwayPipelines(modelDF)\n",
    "\n",
    "          model_home.write.overwrite().save(MODEL_HOME_PATH)\n",
    "          model_away.write.overwrite().save(MODEL_AWAY_PATH)\n",
    "\n",
    "          println(\"[ML] refit complete + models saved\")\n",
    "        } else {\n",
    "          println(\"[ML] skip refit\")\n",
    "        }\n",
    "\n",
    "        // 5) Always re-score + export so the dashboard reacts to new stats\n",
    "        refreshDashboard(seasonLabel = \"25/26\", mcSims = LIVE_MC_SIMS)\n",
    "\n",
    "        // 6) Print key views for demo\n",
    "        if (spark.catalog.tableExists(\"dashboard_live\")) {\n",
    "          println(\"[DEMO] dashboard_live (top 10)\")\n",
    "          spark.table(\"dashboard_live\").show(10, truncate = false)\n",
    "        }\n",
    "        if (spark.catalog.tableExists(\"spi_live\")) {\n",
    "          println(\"[DEMO] spi_live (top 10)\")\n",
    "          spark.table(\"spi_live\").orderBy(desc(\"spi\")).show(10, truncate = false)\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  // NO CHECKPOINTING (demo mode)\n",
    "  .start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8f9ae8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "// -----------------------------\n",
    "// Debug helpers (streams) — SAFE for Toree/REPL\n",
    "// -----------------------------\n",
    "import java.nio.file.{Files, Paths}\n",
    "import scala.collection.JavaConverters._\n",
    "\n",
    "def listFiles(path: String): Unit = {\n",
    "  val p = Paths.get(path)\n",
    "  if (!Files.exists(p)) {\n",
    "    println(s\"[FS] missing: $path\")\n",
    "    return\n",
    "  }\n",
    "  val files = Files.list(p).iterator().asScala.toSeq\n",
    "    .map(_.getFileName.toString)\n",
    "    .sorted\n",
    "  println(s\"[FS] $path (${files.size} files)\")\n",
    "  files.foreach(f => println(s\"  - $f\"))\n",
    "}\n",
    "\n",
    "println(\"\\n[DEBUG] stream input folders\")\n",
    "listFiles(\"data/stream_in/event\")\n",
    "listFiles(\"data/stream_in/stats\")\n",
    "\n",
    "println(\"\\n[DEBUG] active streaming queries\")\n",
    "val act = spark.streams.active\n",
    "println(s\"active=${act.length}\")\n",
    "act.foreach { q =>\n",
    "  println(s\"- name=${q.name} | id=${q.id} | isActive=${q.isActive}\")\n",
    "  println(s\"  status=${q.status}\")\n",
    "  println(s\"  lastProgress=${q.lastProgress}\")\n",
    "}\n",
    "\n",
    "println(\"\\n[DEBUG] current_table_live\")\n",
    "if (spark.catalog.tableExists(\"current_table_live\")) {\n",
    "  spark.table(\"current_table_live\").show(20, truncate = false)\n",
    "} else {\n",
    "  println(\"current_table_live not created yet\")\n",
    "}\n",
    "\n",
    "println(\"\\n[DEBUG] dashboard_live / spi_live\")\n",
    "if (spark.catalog.tableExists(\"dashboard_live\")) spark.table(\"dashboard_live\").show(10, truncate = false)\n",
    "if (spark.catalog.tableExists(\"spi_live\")) spark.table(\"spi_live\").orderBy(desc(\"spi\")).show(10, truncate = false)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf61b273-d1f9-4659-9f17-bc9bc25711f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelDF = [season_id: int, season_label: string ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[season_id: int, season_label: string ... 8 more fields]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (Removed) Legacy batch training path.\n",
    "// Pattern B uses `refreshDashboard()` (called from the stats stream) which loads/trains models and exports outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bf3752-553a-45cc-8067-01bf2ae983cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches used for ML: 253\n",
      "+------------+--------+-----------------+------------------+-------+-------+--------+\n",
      "|season_label|event_id|home_team        |away_team         |home_xg|away_xg|home_adv|\n",
      "+------------+--------+-----------------+------------------+-------+-------+--------+\n",
      "|2023        |11041385|Albirex Niigata  |Tampines Rovers   |0.59   |0.94   |1.0     |\n",
      "|2023        |11041331|Lion City Sailors|Hougang Utd       |3.28   |0.0    |1.0     |\n",
      "|2023        |11041349|Hougang Utd      |Tanjong Pagar Utd.|2.73   |1.12   |1.0     |\n",
      "|2024        |12246956|Hougang Utd      |Young Lions       |2.6    |2.32   |1.0     |\n",
      "|2023        |11041318|Geylang Int.     |Balestier Khalsa  |0.88   |3.69   |1.0     |\n",
      "+------------+--------+-----------------+------------------+-------+-------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "m = [season_label: string, event_id: int ... 5 more fields]\n",
       "home_idx = strIdx_a0e2e18b767b\n",
       "away_idx = strIdx_67ebaae09169\n",
       "enc = oneHotEncoder_57eb833f8dbd\n",
       "assembler = VectorAssembler: uid=vecAssembler_b6a76e6e1533, handleInvalid=error, numInputCols=3\n",
       "homeIdxModel = StringIndexerModel: uid=strIdx_a0e2e18b767b, handleInvalid=keep\n",
       "m1 = [season_label: string, event_id: int ... 6 more fields]\n",
       "awayIdxModel = StringIndexerModel: uid=strIdx_67ebaae09169, handleInvali...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "StringIndexerModel: uid=strIdx_67ebaae09169, handleInvali..."
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (Removed) Legacy batch feature engineering + model fitting setup.\n",
    "// Pattern B handles training and scoring inside `refreshDashboard()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4253a139-6259-44c7-9f17-c025e68084d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glm_home = glm_b0d96b0ba71a\n",
       "glm_away = glm_3ed6c7a335de\n",
       "model_home = GeneralizedLinearRegressionModel: uid=glm_b0d96b0ba71a, family=gaussian, link=identity, numFeatures=19\n",
       "model_away = GeneralizedLinearRegressionModel: uid=glm_3ed6c7a335de, family=gaussian, link=identity, numFeatures=19\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GeneralizedLinearRegressionModel: uid=glm_3ed6c7a335de, family=gaussian, link=identity, numFeatures=19"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (Removed) Legacy batch model fit.\n",
    "// Pattern B trains (or loads) pipelines via `loadOrTrainPipelines()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c883d481-8471-42db-afcc-76947d4ed00e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "coeff = Array(-0.3462127058516612, -0.19809177233738684, 0.5997640074519133, 0.4127271732976447, 0.29010120572657216, -0.4256348720014847, 0.2259891343524253, -0.2273549425913689, -0.35784129032393935, -0.40425119602856974, -0.1504959112002591, 0.18859830123442675, 0.05004954670406097, -0.5807843475125979, -0.7106415271659244, 0.583263704999385, 0.16928048497301232, 0.9544458971491501, 0.0)\n",
       "homeCats = 10\n",
       "awayCats = 10\n",
       "dropLast = true\n",
       "homeDim = 9\n",
       "awayDim = 9\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (Removed) Legacy batch coefficient slicing.\n",
    "// Pattern B computes power rank from the saved pipeline in `powerRankFromHomePipeline()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684e07b3-04cd-49b9-8fc7-eea07577f8e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "homeAttackCoef = Array(-0.3462127058516612, -0.19809177233738684, 0.5997640074519133, 0.4127271732976447, 0.29010120572657216, -0.4256348720014847, 0.2259891343524253, -0.2273549425913689, -0.35784129032393935)\n",
       "awayDefenseCoef = Array(-0.40425119602856974, -0.1504959112002591, 0.18859830123442675, 0.05004954670406097, -0.5807843475125979, -0.7106415271659244, 0.583263704999385, 0.16928048497301232, 0.9544458971491501)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(-0.40425119602856974, -0.1504959112002591, 0.18859830123442675, 0.05004954670406097, -0.5807843475125979, -0.7106415271659244, 0.583263704999385, 0.16928048497301232, 0.9544458971491501)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (Removed) Legacy batch coefficient slicing (home/away blocks).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33a74a0-53db-4abd-9b3c-2ffce35b57eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "homeTeams = Array(Tanjong Pagar Utd., Hougang Utd, Lion City Sailors, Tampines Rovers, Balestier Khalsa, Young Lions, Albirex Niigata, Geylang Int., DPMM, UNKNOWN)\n",
       "awayTeams = Array(Albirex Niigata, Geylang Int., Balestier Khalsa, Hougang Utd, Lion City Sailors, Tampines Rovers, Young Lions, DPMM, Tanjong Pagar Utd., UNKNOWN)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "warning: four deprecations (since 3.0.0); for details, enable `:setting -deprecation' or `:replay -deprecation'\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Array(Albirex Niigata, Geylang Int., Balestier Khalsa, Hougang Utd, Lion City Sailors, Tampines Rovers, Young Lions, DPMM, Tanjong Pagar Utd., UNKNOWN)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (Removed) Legacy batch team-name recovery.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2d2d37-4b7f-4acc-be1c-1cc4200a48d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "homeAttack = ArraySeq((Tanjong Pagar Utd.,-0.3462127058516612), (Hougang Utd,-0.19809177233738684), (Lion City Sailors,0.5997640074519133), (Tampines Rovers,0.4127271732976447), (Balestier Khalsa,0.29010120572657216), (Young Lions,-0.4256348720014847), (Albirex Niigata,0.2259891343524253), (Geylang Int.,-0.2273549425913689), (DPMM,-0.35784129032393935), (UNKNOWN,0.0))\n",
       "awayDefenseWeak = ArraySeq((Albirex Niigata,-0.40425119602856974), (Geylang Int.,-0.1504959112002591), (Balestier Khalsa,0.18859830123442675), (Hougang Utd,0.05004954670406097), (Lion City Sailors,-0.5807843475125979), (Tampines Rovers,-0.7106415271659244), (Young Lions,0.5832637...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "attachBaseline: (teams: Array[String], coef: Array[Double], dropLast: Boolean)Seq[(String, Double)]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "ArraySeq((Albirex Niigata,-0.40425119602856974), (Geylang Int.,-0.1504959112002591), (Balestier Khalsa,0.18859830123442675), (Hougang Utd,0.05004954670406097), (Lion City Sailors,-0.5807843475125979), (Tampines Rovers,-0.7106415271659244), (Young Lions,0.5832637..."
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (Removed) Legacy batch baseline attachment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee11ef-a635-4236-b689-d9ca3f4c2606",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "powerRank = [team: string, home_attack: double ... 2 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------------+--------------------+--------------------+\n",
      "|team              |home_attack         |away_def_weak       |power_score         |\n",
      "+------------------+--------------------+--------------------+--------------------+\n",
      "|Lion City Sailors |0.5997640074519133  |-0.5807843475125979 |1.180548354964511   |\n",
      "|Tampines Rovers   |0.4127271732976447  |-0.7106415271659244 |1.123368700463569   |\n",
      "|Albirex Niigata   |0.2259891343524253  |-0.40425119602856974|0.630240330380995   |\n",
      "|Balestier Khalsa  |0.29010120572657216 |0.18859830123442675 |0.10150290449214541 |\n",
      "|UNKNOWN           |0.0                 |0.0                 |0.0                 |\n",
      "|Geylang Int.      |-0.2273549425913689 |-0.1504959112002591 |-0.0768590313911098 |\n",
      "|Hougang Utd       |-0.19809177233738684|0.05004954670406097 |-0.24814131904144782|\n",
      "|DPMM              |-0.35784129032393935|0.16928048497301232 |-0.5271217752969517 |\n",
      "|Young Lions       |-0.4256348720014847 |0.583263704999385   |-1.0088985770008696 |\n",
      "|Tanjong Pagar Utd.|-0.3462127058516612 |0.9544458971491501  |-1.3006586030008114 |\n",
      "+------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[team: string, home_attack: double ... 2 more fields]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (Removed) Legacy batch power ranking.\n",
    "// Pattern B computes and uses power ranking inside `refreshDashboard()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edabe286-281f-48ef-adc1-5a654dea8b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "futureFixtures = [event_id: int, match_date: date ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[event_id: int, match_date: date ... 4 more fields]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (Removed) Legacy batch future fixture selection.\n",
    "// Pattern B scores fixtures inside `refreshDashboard()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a333d58e-164f-4056-a3d5-5aa8d192fb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "futureFixtures2 = [event_id: int, match_date: date ... 3 more fields]\n",
       "futureAssembled = [event_id: int, match_date: date ... 8 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[event_id: int, match_date: date ... 8 more fields]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (Removed) Legacy batch feature assembly for future fixtures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c80f3ab9-c2de-4f48-ad72-4f5b6e6e45fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "predHome = [event_id: int, match_date: date ... 9 more fields]\n",
       "predBoth = [event_id: int, match_date: date ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+------------------+------------------+------------------+------------------+\n",
      "|event_id|match_date|home_team         |away_team         |lambda_home       |lambda_away       |\n",
      "+--------+----------+------------------+------------------+------------------+------------------+\n",
      "|14195506|2026-01-09|Geylang Int.      |Balestier Khalsa  |1.8227367236523886|2.276027950511678 |\n",
      "|14195507|2026-01-10|Albirex Niigata   |Young Lions       |2.670746204361141 |0.7557585298403504|\n",
      "|14195508|2026-01-11|Tampines Rovers   |Lion City Sailors |1.6934361907943776|1.8037359017997083|\n",
      "|14195509|2026-01-12|Tanjong Pagar Utd.|Hougang Utd       |1.5653302058617307|1.710929538424973 |\n",
      "|14195510|2026-01-16|Geylang Int.      |Hougang Utd       |1.684187969122023 |1.3975322414158229|\n",
      "|14195512|2026-01-18|Tampines Rovers   |Albirex Niigata   |1.8699693422784058|0.9954425233618583|\n",
      "|14195513|2026-01-19|Lion City Sailors |Tanjong Pagar Utd.|3.415703269610394 |0.8999471804676848|\n",
      "|14195514|2026-01-23|Geylang Int.      |Tampines Rovers   |0.9234968952520375|1.9637083993833317|\n",
      "|14195519|2026-01-31|Balestier Khalsa  |Tampines Rovers   |1.4409530435699787|2.305476995481599 |\n",
      "|14195522|2026-01-17|Albirex Niigata   |Tanjong Pagar Utd.|3.0419283965109063|0.8626530328662619|\n",
      "+--------+----------+------------------+------------------+------------------+------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[event_id: int, match_date: date ... 4 more fields]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (Removed) Legacy batch scoring of future fixtures.\n",
    "// Pattern B scores and exports inside `refreshDashboard()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c11855a-0544-4118-9dd8-fca3274fa40e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined class WDL\n",
       "wdlUdf = SparkUserDefinedFunction($Lambda$7766/0x00000001029eb040@602ffc9a,StructType(StructField(_1,DoubleType,false),StructField(_2,DoubleType,false),StructField(_3,DoubleType,false)),List(Some(class[value[0]: double]), Some(class[value[0]: double])),Some(class[_1[0]: double, _2[0]: double, _3[0]: double]),None,true,true)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "poissonPmf: (k: Int, lambda: Double)Double\n",
       "wdlFromLambdas: (lhRaw: Double, laRaw: Double, maxGoals: Int)WDL\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "SparkUserDefinedFunction($Lambda$7766/0x00000001029eb040@602ffc9a,StructType(StructField(_1,DoubleType,false),StructField(_2,DoubleType,false),StructField(_3,DoubleType,false)),List(Some(class[value[0]: double]), Some(class[value[0]: double])),Some(class[_1[0]: double, _2[0]: double, _3[0]: double]),None,true,true)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (Removed) Legacy batch W/D/L UDF.\n",
    "// Pattern B computes W/D/L and expected points inside `refreshDashboard()`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ded6a8-baf1-4edc-be92-82a6b21c1ebc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matchProbs = [event_id: int, match_date: date ... 9 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[event_id: int, match_date: date ... 9 more fields]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (Removed) Legacy batch `matchProbs` derivation.\n",
    "// Pattern B exports match-level fixtures inside `refreshDashboard()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb475a8-66b2-422b-b665-163e1ed58a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+--------------+------------+\n",
      "|team              |win_league_pct|make_acl_pct|\n",
      "+------------------+--------------+------------+\n",
      "|Lion City Sailors |96.64         |99.89       |\n",
      "|Tampines Rovers   |3.145         |76.33       |\n",
      "|Albirex Niigata   |0.2           |20.21       |\n",
      "|Balestier Khalsa  |0.01          |1.525       |\n",
      "|Geylang Int.      |0.005         |2.045       |\n",
      "|Hougang Utd       |0.0           |0.0         |\n",
      "|Tanjong Pagar Utd.|0.0           |0.0         |\n",
      "|Young Lions       |0.0           |0.0         |\n",
      "+------------------+--------------+------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defined class TeamState\n",
       "teams = Array(Lion City Sailors, Geylang Int., Tampines Rovers, Balestier Khalsa, Albirex Niigata, Hougang Utd, Tanjong Pagar Utd., Young Lions)\n",
       "baseTable = Map(Lion City Sailors -> TeamState(15,22,2), Balestier Khalsa -> TeamState(8,9,12), Geylang Int. -> TeamState(10,7,4), Hougang Utd -> TeamState(3,4,10), Tanjong Pagar Utd. -> TeamState(3,7,15), Young Lions -> TeamState(0,4,17), Tampines Rovers -> TeamState(8,8,4), Albirex Niigata -> TeamState(7,8,5))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "poisson: (lambdaRaw: Double, rng: scala.util.Random)Int\n",
       "goalDiff: (s: TeamState)Int\n",
       "simulateSeasonOnce: (teams: Array[String], base: Map[String,TeamState], matches: Array[(String, String, Double, Double)], seed: Int)Map[String,TeamState]\n",
       "matchesArr: A...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Map(Lion City Sailors -> TeamState(15,22,2), Balestier Khalsa -> TeamState(8,9,12), Geylang Int. -> TeamState(10,7,4), Hougang Utd -> TeamState(3,4,10), Tanjong Pagar Utd. -> TeamState(3,7,15), Young Lions -> TeamState(0,4,17), Tampines Rovers -> TeamState(8,8,4), Albirex Niigata -> TeamState(7,8,5))"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (Removed) Legacy batch Monte Carlo simulation.\n",
    "// Pattern B runs MC and exports probabilities inside `refreshDashboard()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c61bc7-a207-48fa-a7ba-9e451e0fbc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "// (Removed) Legacy batch MC aggregation to probsDF.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac00551-ec18-410a-a402-a8803f185a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "w = org.apache.spark.sql.expressions.WindowSpec@1de69208\n",
       "spiDF = [team: string, spi: double]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[team: string, spi: double]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (Removed) Legacy batch SPI computation.\n",
    "// Pattern B computes SPI inside `refreshDashboard()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ff8309-62ef-479f-8e70-5eb5c9ef16e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+---+---+--------------------+----------------+--------------+------------+------------------+\n",
      "|team              |pts |gf |ga |exp_pts_remaining_mc|exp_pts_final_mc|win_league_pct|make_acl_pct|spi               |\n",
      "+------------------+----+---+---+--------------------+----------------+--------------+------------+------------------+\n",
      "|Lion City Sailors |15.0|22 |2  |37.7883             |52.7883         |96.64         |99.89       |89.47321462247646 |\n",
      "|Tampines Rovers   |8.0 |8  |4  |31.90965            |39.90965        |3.145         |76.33       |88.77961094002528 |\n",
      "|Albirex Niigata   |7.0 |8  |5  |27.139200000000002  |34.1392         |0.2           |20.21       |82.79783898992036 |\n",
      "|Geylang Int.      |10.0|7  |4  |17.8014             |27.8014         |0.005         |2.045       |74.22054475013205 |\n",
      "|Balestier Khalsa  |8.0 |9  |12 |18.26305            |26.26305        |0.01          |1.525       |76.38412017949582 |\n",
      "|Hougang Utd       |3.0 |4  |10 |13.7406             |16.7406         |0.0           |0.0         |72.14284724583914 |\n",
      "|Tanjong Pagar Utd.|3.0 |7  |15 |11.3172             |14.3172         |0.0           |0.0         |59.37554617748956 |\n",
      "|Young Lions       |0.0 |4  |17 |8.3676              |8.3676          |0.0           |0.0         |62.914669143373864|\n",
      "+------------------+----+---+---+--------------------+----------------+--------------+------------+------------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "core = [team: string, pts: double ... 2 more fields]\n",
       "mcPts = [team: string, exp_pts_final_mc: double]\n",
       "mcRem = [team: string, exp_pts_remaining_mc: double]\n",
       "odds = [team: string, win_league_pct: double ... 1 more field]\n",
       "spi = [team: string, spi: double]\n",
       "finalTable = [team: string, pts: double ... 7 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[team: string, pts: double ... 7 more fields]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (Removed) Legacy batch finalTable join/show.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b28b437-e623-42f9-8b29-3382da3eb343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dash = [team: string, pts: double ... 4 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[team: string, pts: double ... 4 more fields]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (Removed) Legacy batch dashboard CSV export.\n",
    "// Pattern B writes dashboard CSVs inside `refreshDashboard()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1440742f-6188-4d71-9441-01846e9e340b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "homeView = [event_id: int, match_date: date ... 9 more fields]\n",
       "awayView = [event_id: int, match_date: date ... 9 more fields]\n",
       "teamFixtures = [event_id: int, match_date: date ... 9 more fields]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[event_id: int, match_date: date ... 9 more fields]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// (Removed) Legacy batch match_level_fixtures export.\n",
    "// Pattern B writes fixtures CSVs inside `refreshDashboard()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117cafda",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "// -----------------------------\n",
    "// DEMO RESET (recommended instead of disabling checkpoints)\n",
    "// -----------------------------\n",
    "// Use this before a demo so the professor can drop files and see changes immediately.\n",
    "// It stops the streams and deletes the checkpoint folders (so Spark will re-read files).\n",
    "// Optional: also delete test input files.\n",
    "\n",
    "import org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "def rm(path: String): Unit = {\n",
    "  val fs = FileSystem.get(spark.sparkContext.hadoopConfiguration)\n",
    "  val p = new Path(path)\n",
    "  if (fs.exists(p)) {\n",
    "    fs.delete(p, true)\n",
    "    println(s\"[RM] deleted: $path\")\n",
    "  } else {\n",
    "    println(s\"[RM] missing: $path\")\n",
    "  }\n",
    "}\n",
    "\n",
    "def stopQuery(name: String, qOpt: => Any): Unit = {\n",
    "  try {\n",
    "    val q = qOpt.asInstanceOf[org.apache.spark.sql.streaming.StreamingQuery]\n",
    "    if (q != null && q.isActive) {\n",
    "      q.stop()\n",
    "      println(s\"[STOP] $name stopped\")\n",
    "    } else {\n",
    "      println(s\"[STOP] $name not active\")\n",
    "    }\n",
    "  } catch {\n",
    "    case _: Throwable => println(s\"[STOP] $name not found\")\n",
    "  }\n",
    "}\n",
    "\n",
    "// 1) Stop streams (ignore if not running)\n",
    "stopQuery(\"event stream (q)\", q)\n",
    "stopQuery(\"stats stream (mlQ)\", mlQ)\n",
    "\n",
    "// 2) Clear checkpoints\n",
    "rm(\"data/_chk/current_table_live\")\n",
    "rm(\"data/_chk/rerun_model_on_stats\")\n",
    "\n",
    "// 3) Optional: clear input folders so you only process fresh demo files\n",
    "// Uncomment if desired.\n",
    "// rm(\"data/stream_in/event\")\n",
    "// rm(\"data/stream_in/stats\")\n",
    "\n",
    "println(\"[DEMO RESET] done. Now restart Cell 8 and Cell 12, then drop new JSON files.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d475350",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "// -----------------------------\n",
    "// Show SPI output (after refreshDashboard has run)\n",
    "// -----------------------------\n",
    "if (spark.catalog.tableExists(\"spi_live\")) {\n",
    "  spark.table(\"spi_live\").orderBy(desc(\"spi\")).show(50, truncate = false)\n",
    "} else {\n",
    "  println(\"spi_live not found. Run refreshDashboard(...) first.\")\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c756a7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "// -----------------------------\n",
    "// Show dashboard + fixtures outputs (after refreshDashboard has run)\n",
    "// -----------------------------\n",
    "if (spark.catalog.tableExists(\"dashboard_live\")) {\n",
    "  spark.table(\"dashboard_live\").show(50, truncate = false)\n",
    "} else {\n",
    "  println(\"dashboard_live not found. Run refreshDashboard(...) first.\")\n",
    "}\n",
    "\n",
    "if (spark.catalog.tableExists(\"fixtures_live\")) {\n",
    "  spark.table(\"fixtures_live\").orderBy(\"match_date\").show(30, truncate = false)\n",
    "} else {\n",
    "  println(\"fixtures_live not found. Run refreshDashboard(...) first.\")\n",
    "}\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Apache Toree - Scala",
   "language": "scala",
   "name": "apache_toree_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
